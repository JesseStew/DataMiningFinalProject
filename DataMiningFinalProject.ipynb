{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "https://www.learndatasci.com/tutorials/sentiment-analysis-reddit-headlines-pythons-nltk/\n",
    "Grammar Analysis\n",
    "\n",
    "    Possibly compare to other accounts (or on English grammar dataset)\n",
    "        Try to figure out what their first language is.\n",
    "        \n",
    "    % of posts and comments at subreddit, maybe per account per subreddit\n",
    "        Where they are posting and commenting\n",
    "        \n",
    "    Some similarity between post/comment content and karma (upvotes/downvotes)\n",
    "        What posts gained traction\n",
    "        \n",
    "    If more than x upvotes, some type of analysis, similarity\n",
    "        Search for any claims of where they are from.\n",
    "        \n",
    "    Possibly use findings to try and detect unknown suspicious accounts\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A hard look at training and tactics\" = They will be sent more $$$ for \"training\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullname</th>\n",
       "      <th>archived</th>\n",
       "      <th>author.name</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>downs</th>\n",
       "      <th>edited</th>\n",
       "      <th>...</th>\n",
       "      <th>link_url</th>\n",
       "      <th>name</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>permalink</th>\n",
       "      <th>score</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit.display_name_prefixed</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>ups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t1_d687zh5</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A hard look at training and tactics\" = They wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.470604e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>http://www.reuters.com/article/us-usa-police-c...</td>\n",
       "      <td>t1_d687zh5</td>\n",
       "      <td>119</td>\n",
       "      <td>t3_4wkn7m</td>\n",
       "      <td>/r/Bad_Cop_No_Donut/comments/4wkn7m/chicago_po...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Bad_Cop_No_Donut</td>\n",
       "      <td>public</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t1_d5wqzhx</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>They deserve all of the hate</td>\n",
       "      <td>0</td>\n",
       "      <td>1.469847e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>http://reason.com/blog/2016/07/28/pine-bluff-c...</td>\n",
       "      <td>t1_d5wqzhx</td>\n",
       "      <td>96</td>\n",
       "      <td>t3_4v5xpc</td>\n",
       "      <td>/r/Bad_Cop_No_Donut/comments/4v5xpc/arkansas_p...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Bad_Cop_No_Donut</td>\n",
       "      <td>public</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t1_d5qvqfw</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I guess that's what they mean when say \"I don'...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.469498e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>http://countercurrentnews.com/2016/07/no-charg...</td>\n",
       "      <td>t1_d5qvqfw</td>\n",
       "      <td>210</td>\n",
       "      <td>t1_d5qeyrw</td>\n",
       "      <td>/r/Bad_Cop_No_Donut/comments/4uiezg/no_charges...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Bad_Cop_No_Donut</td>\n",
       "      <td>public</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t1_d5quz9y</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It's never too late for them, It's never too c...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.469497e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>http://www.opposingviews.com/i/society/police-...</td>\n",
       "      <td>t1_d5quz9y</td>\n",
       "      <td>18</td>\n",
       "      <td>t3_4uicjv</td>\n",
       "      <td>/r/Bad_Cop_No_Donut/comments/4uicjv/police_off...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Bad_Cop_No_Donut</td>\n",
       "      <td>public</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t1_d565ls1</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://petitions.whitehouse.gov//petition/pet...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.468114e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>http://www.thelibertyconservative.com/favor-po...</td>\n",
       "      <td>t1_d565ls1</td>\n",
       "      <td>12</td>\n",
       "      <td>t1_d55o1gr</td>\n",
       "      <td>/r/Good_Cop_Free_Donut/comments/4s0s3j/you_can...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Good_Cop_Free_Donut</td>\n",
       "      <td>public</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     fullname  archived  author.name author_flair_text  \\\n",
       "0  t1_d687zh5      True  BlackToLive               NaN   \n",
       "1  t1_d5wqzhx      True  BlackToLive               NaN   \n",
       "2  t1_d5qvqfw      True  BlackToLive               NaN   \n",
       "3  t1_d5quz9y      True  BlackToLive               NaN   \n",
       "4  t1_d565ls1      True  BlackToLive               NaN   \n",
       "\n",
       "                                                body  controversiality  \\\n",
       "0  A hard look at training and tactics\" = They wi...                 0   \n",
       "1                       They deserve all of the hate                 0   \n",
       "2  I guess that's what they mean when say \"I don'...                 0   \n",
       "3  It's never too late for them, It's never too c...                 0   \n",
       "4  https://petitions.whitehouse.gov//petition/pet...                 0   \n",
       "\n",
       "    created_utc  distinguished  downs edited ...  \\\n",
       "0  1.470604e+09            NaN      0  False ...   \n",
       "1  1.469847e+09            NaN      0  False ...   \n",
       "2  1.469498e+09            NaN      0  False ...   \n",
       "3  1.469497e+09            NaN      0  False ...   \n",
       "4  1.468114e+09            NaN      0  False ...   \n",
       "\n",
       "                                            link_url        name  \\\n",
       "0  http://www.reuters.com/article/us-usa-police-c...  t1_d687zh5   \n",
       "1  http://reason.com/blog/2016/07/28/pine-bluff-c...  t1_d5wqzhx   \n",
       "2  http://countercurrentnews.com/2016/07/no-charg...  t1_d5qvqfw   \n",
       "3  http://www.opposingviews.com/i/society/police-...  t1_d5quz9y   \n",
       "4  http://www.thelibertyconservative.com/favor-po...  t1_d565ls1   \n",
       "\n",
       "   num_comments   parent_id  \\\n",
       "0           119   t3_4wkn7m   \n",
       "1            96   t3_4v5xpc   \n",
       "2           210  t1_d5qeyrw   \n",
       "3            18   t3_4uicjv   \n",
       "4            12  t1_d55o1gr   \n",
       "\n",
       "                                           permalink score stickied  \\\n",
       "0  /r/Bad_Cop_No_Donut/comments/4wkn7m/chicago_po...     1    False   \n",
       "1  /r/Bad_Cop_No_Donut/comments/4v5xpc/arkansas_p...     1    False   \n",
       "2  /r/Bad_Cop_No_Donut/comments/4uiezg/no_charges...     1    False   \n",
       "3  /r/Bad_Cop_No_Donut/comments/4uicjv/police_off...     1    False   \n",
       "4  /r/Good_Cop_Free_Donut/comments/4s0s3j/you_can...     1    False   \n",
       "\n",
       "  subreddit.display_name_prefixed subreddit_type ups  \n",
       "0              r/Bad_Cop_No_Donut         public   1  \n",
       "1              r/Bad_Cop_No_Donut         public   1  \n",
       "2              r/Bad_Cop_No_Donut         public   1  \n",
       "3              r/Bad_Cop_No_Donut         public   1  \n",
       "4           r/Good_Cop_Free_Donut         public   1  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "dataframe_comments = pd.read_csv('reddit-suspicious-accounts/data/comments.csv', encoding='utf-8')\n",
    "\n",
    "#print(dataframe_comments['body'][0])\n",
    "dataframe_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6711\n"
     ]
    }
   ],
   "source": [
    "#Tokenizes Comments for Sentiment analysis\n",
    "tokenized_comments = []\n",
    "for itr in dataframe_comments.index.values:\n",
    "    tokenized_comments.append(nltk.word_tokenize(dataframe_comments['body'][itr], language = 'English', preserve_line = 'True'))\n",
    "\n",
    "print(len(tokenized_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "#Comments Containing the words Russia or russian\n",
    "russia_reference = []\n",
    "russian_reference = []\n",
    "for itr in tokenized_comments:\n",
    "    for num in range(0, len(itr)):\n",
    "        if itr[num] == 'Russia' or itr[num] == 'russia': \n",
    "            russia_reference.append(itr)\n",
    "        if itr[num] == 'Russian' or itr[num] == 'russian':\n",
    "            russian_reference.append(itr)\n",
    "\n",
    "print(len(russia_reference))\n",
    "print(len(russian_reference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullname</th>\n",
       "      <th>archived</th>\n",
       "      <th>author.name</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>downs</th>\n",
       "      <th>edited</th>\n",
       "      <th>...</th>\n",
       "      <th>link_url</th>\n",
       "      <th>name</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>permalink</th>\n",
       "      <th>score</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit.display_name_prefixed</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>ups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>t1_d509t9i</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>By submitting to an independent, non-profit co...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.467742e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>http://blacktolive.org/?p=9589</td>\n",
       "      <td>t1_d509t9i</td>\n",
       "      <td>7</td>\n",
       "      <td>t1_d508xpp</td>\n",
       "      <td>/r/politics/comments/4rdu5x/hillarys_war_on_bl...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>r/politics</td>\n",
       "      <td>public</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>t1_crya21g</td>\n",
       "      <td>True</td>\n",
       "      <td>FaurnFlamebreaker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Well, in this country it's either you are top ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.433665e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>http://www.npr.org/2015/06/06/412314705/jury-a...</td>\n",
       "      <td>t1_crya21g</td>\n",
       "      <td>287</td>\n",
       "      <td>t3_38v84b</td>\n",
       "      <td>/r/worldnews/comments/38v84b/jury_acquits_exbp...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>r/worldnews</td>\n",
       "      <td>public</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>t1_cry9m2s</td>\n",
       "      <td>True</td>\n",
       "      <td>FaurnFlamebreaker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&gt; large-scale military exercises near Russia\\r...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.433664e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>http://freebeacon.com/national-security/stratc...</td>\n",
       "      <td>t1_cry9m2s</td>\n",
       "      <td>30</td>\n",
       "      <td>t3_38vt0w</td>\n",
       "      <td>/r/worldnews/comments/38vt0w/stratcom_deploys_...</td>\n",
       "      <td>-6</td>\n",
       "      <td>False</td>\n",
       "      <td>r/worldnews</td>\n",
       "      <td>public</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>t1_cv3psup</td>\n",
       "      <td>True</td>\n",
       "      <td>Clawisma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>That's the best I have seen today!\\r\\ncrying w...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.442426e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>http://i.imgur.com/mAbIDsm.gifv</td>\n",
       "      <td>t1_cv3psup</td>\n",
       "      <td>2129</td>\n",
       "      <td>t3_3l64el</td>\n",
       "      <td>/r/aww/comments/3l64el/wife_surprises_him_for_...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>r/aww</td>\n",
       "      <td>public</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>t1_dsije4z</td>\n",
       "      <td>False</td>\n",
       "      <td>picnicshirt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This is a pro-BitcoinCash article hosted on bi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.515670e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>https://news.bitcoin.com/miami-bitcoin-confere...</td>\n",
       "      <td>t1_dsije4z</td>\n",
       "      <td>47</td>\n",
       "      <td>t3_7pigps</td>\n",
       "      <td>/r/Buttcoin/comments/7pigps/miami_bitcoin_conf...</td>\n",
       "      <td>-2</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Buttcoin</td>\n",
       "      <td>public</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fullname  archived        author.name author_flair_text  \\\n",
       "5   t1_d509t9i      True        BlackToLive               NaN   \n",
       "10  t1_crya21g      True  FaurnFlamebreaker               NaN   \n",
       "11  t1_cry9m2s      True  FaurnFlamebreaker               NaN   \n",
       "37  t1_cv3psup      True           Clawisma               NaN   \n",
       "93  t1_dsije4z     False        picnicshirt               NaN   \n",
       "\n",
       "                                                 body  controversiality  \\\n",
       "5   By submitting to an independent, non-profit co...                 1   \n",
       "10  Well, in this country it's either you are top ...                 1   \n",
       "11  > large-scale military exercises near Russia\\r...                 1   \n",
       "37  That's the best I have seen today!\\r\\ncrying w...                 1   \n",
       "93  This is a pro-BitcoinCash article hosted on bi...                 1   \n",
       "\n",
       "     created_utc  distinguished  downs edited ...  \\\n",
       "5   1.467742e+09            NaN      0  False ...   \n",
       "10  1.433665e+09            NaN      0  False ...   \n",
       "11  1.433664e+09            NaN      0  False ...   \n",
       "37  1.442426e+09            NaN      0  False ...   \n",
       "93  1.515670e+09            NaN      0  False ...   \n",
       "\n",
       "                                             link_url        name  \\\n",
       "5                      http://blacktolive.org/?p=9589  t1_d509t9i   \n",
       "10  http://www.npr.org/2015/06/06/412314705/jury-a...  t1_crya21g   \n",
       "11  http://freebeacon.com/national-security/stratc...  t1_cry9m2s   \n",
       "37                    http://i.imgur.com/mAbIDsm.gifv  t1_cv3psup   \n",
       "93  https://news.bitcoin.com/miami-bitcoin-confere...  t1_dsije4z   \n",
       "\n",
       "    num_comments   parent_id  \\\n",
       "5              7  t1_d508xpp   \n",
       "10           287   t3_38v84b   \n",
       "11            30   t3_38vt0w   \n",
       "37          2129   t3_3l64el   \n",
       "93            47   t3_7pigps   \n",
       "\n",
       "                                            permalink score stickied  \\\n",
       "5   /r/politics/comments/4rdu5x/hillarys_war_on_bl...     0    False   \n",
       "10  /r/worldnews/comments/38v84b/jury_acquits_exbp...     0    False   \n",
       "11  /r/worldnews/comments/38vt0w/stratcom_deploys_...    -6    False   \n",
       "37  /r/aww/comments/3l64el/wife_surprises_him_for_...     1    False   \n",
       "93  /r/Buttcoin/comments/7pigps/miami_bitcoin_conf...    -2    False   \n",
       "\n",
       "   subreddit.display_name_prefixed subreddit_type ups  \n",
       "5                       r/politics         public   0  \n",
       "10                     r/worldnews         public   0  \n",
       "11                     r/worldnews         public  -6  \n",
       "37                           r/aww         public   1  \n",
       "93                      r/Buttcoin         public  -2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#New dataframe with controveriality > 0 printing comments\n",
    "controversial_comments = dataframe_comments.query('controversiality > 0').copy()\n",
    "\n",
    "controversial_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullname</th>\n",
       "      <th>archived</th>\n",
       "      <th>author.name</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>brand_safe</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>domain</th>\n",
       "      <th>downs</th>\n",
       "      <th>edited</th>\n",
       "      <th>...</th>\n",
       "      <th>selftext</th>\n",
       "      <th>spoiler</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit_name_prefixed</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>title</th>\n",
       "      <th>ups</th>\n",
       "      <th>url</th>\n",
       "      <th>whitelist_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_8b7ryg</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1.523369e+09</td>\n",
       "      <td>admin</td>\n",
       "      <td>self.reddit</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>u/reddit</td>\n",
       "      <td>user</td>\n",
       "      <td>0</td>\n",
       "      <td>This account is banned and is temporarily pres...</td>\n",
       "      <td>325</td>\n",
       "      <td>https://www.reddit.com/r/u_reddit/comments/8b7...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_511r3a</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1.472951e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Bad_Cop_No_Donut</td>\n",
       "      <td>public</td>\n",
       "      <td>125644</td>\n",
       "      <td>Cops really be trying hard to take non-black p...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://twitter.com/gloed_up/status/7707843365...</td>\n",
       "      <td>promo_adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_4xyikx</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>1.471337e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abcnews.go.com</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>r/news</td>\n",
       "      <td>public</td>\n",
       "      <td>15884252</td>\n",
       "      <td>Milwaukee Police Chief: Some Arrests Made but ...</td>\n",
       "      <td>1</td>\n",
       "      <td>http://abcnews.go.com/US/wireStory/latest-poli...</td>\n",
       "      <td>all_ads</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_4va15u</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1.469844e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>blacktolive.org</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>r/BlackLivesMatter</td>\n",
       "      <td>public</td>\n",
       "      <td>6676</td>\n",
       "      <td>We Want Real Equality</td>\n",
       "      <td>1</td>\n",
       "      <td>http://blacktolive.org/opinion/we-want-real-eq...</td>\n",
       "      <td>promo_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_4v9u9s</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1.469841e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>blacktolive.org</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Bad_Cop_No_Donut</td>\n",
       "      <td>public</td>\n",
       "      <td>125644</td>\n",
       "      <td>Three Remaining Cops In The Freddie Gray Case ...</td>\n",
       "      <td>1</td>\n",
       "      <td>http://blacktolive.org/news/crime/three-remain...</td>\n",
       "      <td>promo_adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    fullname  archived  author.name author_flair_text  brand_safe  \\\n",
       "0  t3_8b7ryg     False       reddit               NaN       False   \n",
       "1  t3_511r3a      True  BlackToLive               NaN       False   \n",
       "2  t3_4xyikx      True  BlackToLive               NaN        True   \n",
       "3  t3_4va15u      True  BlackToLive               NaN       False   \n",
       "4  t3_4v9u9s      True  BlackToLive               NaN       False   \n",
       "\n",
       "    created_utc distinguished           domain  downs edited  \\\n",
       "0  1.523369e+09         admin      self.reddit      0  False   \n",
       "1  1.472951e+09           NaN      twitter.com      0  False   \n",
       "2  1.471337e+09           NaN   abcnews.go.com      0  False   \n",
       "3  1.469844e+09           NaN  blacktolive.org      0  False   \n",
       "4  1.469841e+09           NaN  blacktolive.org      0  False   \n",
       "\n",
       "         ...         selftext spoiler  stickied  subreddit_name_prefixed  \\\n",
       "0        ...              NaN   False     False                 u/reddit   \n",
       "1        ...              NaN   False     False       r/Bad_Cop_No_Donut   \n",
       "2        ...              NaN   False     False                   r/news   \n",
       "3        ...              NaN   False     False       r/BlackLivesMatter   \n",
       "4        ...              NaN   False     False       r/Bad_Cop_No_Donut   \n",
       "\n",
       "   subreddit_type  subreddit_subscribers  \\\n",
       "0            user                      0   \n",
       "1          public                 125644   \n",
       "2          public               15884252   \n",
       "3          public                   6676   \n",
       "4          public                 125644   \n",
       "\n",
       "                                               title  ups  \\\n",
       "0  This account is banned and is temporarily pres...  325   \n",
       "1  Cops really be trying hard to take non-black p...    1   \n",
       "2  Milwaukee Police Chief: Some Arrests Made but ...    1   \n",
       "3                              We Want Real Equality    1   \n",
       "4  Three Remaining Cops In The Freddie Gray Case ...    1   \n",
       "\n",
       "                                                 url  whitelist_status  \n",
       "0  https://www.reddit.com/r/u_reddit/comments/8b7...               NaN  \n",
       "1  https://twitter.com/gloed_up/status/7707843365...       promo_adult  \n",
       "2  http://abcnews.go.com/US/wireStory/latest-poli...           all_ads  \n",
       "3  http://blacktolive.org/opinion/we-want-real-eq...         promo_all  \n",
       "4  http://blacktolive.org/news/crime/three-remain...       promo_adult  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_submissions = pd.read_csv('reddit-suspicious-accounts/data/submissions.csv', encoding='utf-8')\n",
    "dataframe_submissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullname</th>\n",
       "      <th>comment_karma</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>has_subscribed</th>\n",
       "      <th>has_verified_email</th>\n",
       "      <th>icon_img</th>\n",
       "      <th>id</th>\n",
       "      <th>is_employee</th>\n",
       "      <th>is_gold</th>\n",
       "      <th>is_mod</th>\n",
       "      <th>link_karma</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t2_z919g</td>\n",
       "      <td>-7</td>\n",
       "      <td>1.467681e+09</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>https://www.redditstatic.com/avatars/avatar_de...</td>\n",
       "      <td>z919g</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>BlackToLive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2_nhk4d</td>\n",
       "      <td>-5</td>\n",
       "      <td>1.431683e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.redditstatic.com/avatars/avatar_de...</td>\n",
       "      <td>nhk4d</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>FaurnFlamebreaker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t2_r8rca</td>\n",
       "      <td>-4</td>\n",
       "      <td>1.445054e+09</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.redditstatic.com/avatars/avatar_de...</td>\n",
       "      <td>r8rca</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Bill_Jonson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t2_nhml3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.431694e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.redditstatic.com/avatars/avatar_de...</td>\n",
       "      <td>nhml3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>PurebringerOghmagra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t2_qidqv</td>\n",
       "      <td>-2</td>\n",
       "      <td>1.442424e+09</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.redditstatic.com/avatars/avatar_de...</td>\n",
       "      <td>qidqv</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Clawisma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fullname  comment_karma   created_utc  has_subscribed has_verified_email  \\\n",
       "0  t2_z919g             -7  1.467681e+09            True               True   \n",
       "1  t2_nhk4d             -5  1.431683e+09           False              False   \n",
       "2  t2_r8rca             -4  1.445054e+09            True              False   \n",
       "3  t2_nhml3              0  1.431694e+09           False              False   \n",
       "4  t2_qidqv             -2  1.442424e+09            True              False   \n",
       "\n",
       "                                            icon_img     id  is_employee  \\\n",
       "0  https://www.redditstatic.com/avatars/avatar_de...  z919g        False   \n",
       "1  https://www.redditstatic.com/avatars/avatar_de...  nhk4d        False   \n",
       "2  https://www.redditstatic.com/avatars/avatar_de...  r8rca        False   \n",
       "3  https://www.redditstatic.com/avatars/avatar_de...  nhml3        False   \n",
       "4  https://www.redditstatic.com/avatars/avatar_de...  qidqv        False   \n",
       "\n",
       "   is_gold  is_mod  link_karma                 name  \n",
       "0    False   False           1          BlackToLive  \n",
       "1    False   False           1    FaurnFlamebreaker  \n",
       "2    False   False           1          Bill_Jonson  \n",
       "3    False   False           1  PurebringerOghmagra  \n",
       "4    False   False           1             Clawisma  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_users = pd.read_csv('reddit-suspicious-accounts/data/users.csv', encoding='utf-8')\n",
    "dataframe_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'one': 7, 'terrorist': 7, 'get': 6, 'make': 5, 'like': 5, 'people': 5, 'american': 4, 'wa': 4, 'dont': 4, 'bitcoin': 4, 'time': 4, 'could': 4, 'u': 4, 'even': 4, 'every': 3, 'want': 3, '1': 3, 'never': 3, 'account': 3, 'life': 3, 'next': 3, 'child': 3, 'well': 3, 'think': 3, 'muslim': 3, 'christian': 3, 'way': 3, 'post': 3, 'know': 3, 'cost': 2, 'thats': 2, 'actually': 2, 'feel': 2, 'casting': 2, 'two': 2, 'last': 2, 'movie': 2, 'friend': 2, 'food': 2, '000': 2, 'whats': 2, 'statement': 2, 'r': 2, 'create': 2, 'idea': 2, 'say': 2, 'ill': 2, 'roy': 2, 'moore': 2, 'good': 2, 'isnt': 2, 'meant': 2, 'obama': 2, 'trump': 2, 'call': 2, 'said': 2, 'america': 2, 'attack': 2, 'hall': 2, 'school': 2, 'fix': 2, 'cash': 2, 'old': 2, 'end': 2, 'russian': 2, 'hope': 2, 'killed': 2, 'illness': 2, 'doctor': 2, 'limited': 2, 'supply': 2, 'steal': 2, 'bf': 2, 'unfaithful': 2, 'bye': 2, 'funeral': 1, 'buried': 1, 'healthcare': 1, 'beef': 1, 'jerky': 1, 'delicious': 1, 'generally': 1, 'prohibitive': 1, 'numbness': 1, 'apathy': 1, 'hopeless': 1, 'hard': 1, 'motivate': 1, 'thing': 1, 'sadness': 1, 'separate': 1, 'emotion': 1, 'rarely': 1, 'generic': 1, 'white': 1, 'kid': 1, 'play': 1, 'sokka': 1, 'katara': 1, 'airbender': 1, 'scratch': 1, 'anyone': 1, 'pretty': 1, 'awful': 1, 'racist': 1, 'patient': 1, 'demanded': 1, 'specifically': 1, 'checked': 1, 'caucasian': 1, 'doctorand': 1, 'ethnicity': 1, 'country': 1, 'origin': 1, 'early': 1, '90': 1, 'internet': 1, 'illustration': 1, 'instruction': 1, 'tampon': 1, 'box': 1, 'oldest': 1, 'fucked': 1, 'pet': 1, 'german': 1, 'shepard': 1, 'staying': 1, 'hungry': 1, 'dad': 1, 'wearing': 1, 'mom': 1, 'dress': 1, 'absence': 1, 'proof': 1, 'seriously': 1, 'point': 1, 'unfounded': 1, 'george': 1, 'martin': 1, 'especially': 1, 'finish': 1, 'song': 1, 'ice': 1, 'fire': 1, 'instagram': 1, 'based': 1, 'unique': 1, 'popular': 1, 'money': 1, 'ad': 1, 'single': 1, 'cd': 1, 'entire': 1, 'boxed': 1, 'set': 1, 'note': 1, 'kirov': 1, 'reporting': 1, 'dropped': 1, 'childish': 1, 'gambino': 1, 'redbone': 1, 'beer': 1, 'snob': 1, 'look': 1, 'microbrew': 1, 'sometimes': 1, 'miller': 1, 'high': 1, 'chocolate': 1, 'eggnog': 1, 'stout': 1, 'ok': 1, 'eth': 1, 'change': 1, 'future': 1, 'build': 1, 'foundation': 1, 'dapps': 1, 'grow': 1, 'flourish': 1, 'ether': 1, 'currency': 1, 'doe': 1, 'value': 1, 'always': 1, 'forget': 1, 'quick': 1, 'gainz': 1, 'new': 1, 'paradigm': 1, 'barack': 1, 'donald': 1, 'recorded': 1, 'robocalls': 1, 'alabama': 1, 'senate': 1, 'race': 1, 'former': 1, 'federal': 1, 'prosecutor': 1, 'doug': 1, 'jones': 1, 'accused': 1, 'molester': 1, 'enough': 1, 'alrady': 1, 'great': 1, 'slavery': 1, 'else': 1, 'expect': 1, 'domestic': 1, 'immigrant': 1, 'holding': 1, 'btc': 1, 'extremely': 1, 'nervous': 1, '100': 1, 'main': 1, 'question': 1, 'come': 1, 'mind': 1, 'europe': 1, 'finally': 1, 'tired': 1, 'constant': 1, 'threat': 1, 'agree': 1, 'completely': 1, 'support': 1, 'opinion': 1, 'absolutely': 1, 'besides': 1, 'bad': 1, 'listen': 1, 'carol': 1, 'sitting': 1, 'vise': 1, 'versa': 1, 'visit': 1, 'festival': 1, 'concert': 1, 'true': 1, 'religiously': 1, 'tolerant': 1, 'society': 1, 'banning': 1, 'holiday': 1, 'wiretapping': 1, 'citizen': 1, 'necessary': 1, 'security': 1, 'l': 1, 'form': 1, 'tetri': 1, 'bfg': 1, '9000': 1, 'ridiculous': 1, 'piece': 1, 'nonsense': 1, 'ha': 1, 'involved': 1, 'insupported': 1, '2': 1, 'attempt': 1, 'xt': 1, 'classic': 1, 'oh': 1, 'apart': 1, 'trying': 1, 'gain': 1, 'leaverage': 1, 'conviction': 1, 'selling': 1, 'firecracker': 1, 'making': 1, 'mtgox': 1, 'accurate': 1, 'part': 1, 'poorly': 1, 'disguised': 1, 'program': 1, 'trust': 1, 'roger': 1, 'take': 1, 'idiot': 1, 'rubbish': 1, 'steemit': 1, 'grew': 1, 'execution': 1, 'romanov': 1, 'family': 1, 'july': 1, '1617': 1, '1918': 1, 'empire': 1, 'beginning': 1, 'soviet': 1, 'union': 1, 'guy': 1, 'back': 1, 'mess': 1, 'cat': 1, 'tail': 1, 'lowering': 1, 'strike': 1, 'youre': 1, 'right': 1, 'system': 1, 'corrupt': 1, 'dependable': 1, 'learn': 1, 'defend': 1, 'law': 1, 'realized': 1, 'cop': 1, 'afraid': 1, 'lawyer': 1, 'bring': 1, 'club': 1, 'elementary': 1, 'cant': 1, 'satanist': 1, 'need': 1, 'gun': 1, 'protect': 1, 'property': 1, 'hangover': 1, 'morning': 1, 'without': 1, 'alcohol': 1, 'creating': 1, 'video': 1, 'game': 1, 'angrier': 1, 'playing': 1, 'drill': 1, 'hole': 1, 'iphone': 1, 'hidden': 1, 'headphone': 1, 'socket': 1, 'cover': 1, 'hereby': 1, 'deusxyx': 1, 'promise': 1, 'use': 1, 'smth': 1, 'abbreviation': 1, 'something': 1, 'drive': 1, 'abyss': 1, 'obscurity': 1, 'offer': 1, 'sincere': 1, 'apology': 1, 'rdesignporn': 1, 'member': 1, 'mistake': 1, 'forgiven': 1, 'walk': 1, 'sunset': 1, 'sackcloth': 1, 'ash': 1, 'lie': 1, 'calmly': 1, 'baby': 1, 'breastfeeds': 1, 'caress': 1, 'im': 1, 'enjoying': 1, 'soon': 1, 'picture': 1, 'pose': 1, 'used': 1, 'obituary': 1, 'first': 1, 'firefighter': 1, 'responding': 1, '911': 1, 'struck': 1, 'dead': 1, 'courtyard': 1, 'falling': 1, 'body': 1, 'simultaneously': 1, 'huge': 1, 'amount': 1, 'arent': 1, 'curable': 1, 'treatable': 1, 'go': 1, 'find': 1, 'wrong': 1, 'many': 1, 'throw': 1, 'hand': 1, 'causing': 1, 'unwell': 1, 'often': 1, 'year': 1, 'become': 1, 'astronaut': 1, 'apocalyptic': 1, 'asteroid': 1, 'hit': 1, 'among': 1, 'human': 1, 'left': 1, 'alive': 1, 'oxygen': 1, 'external': 1, 'assistance': 1, 'returning': 1, 'home': 1, 'surviving': 1, 'may': 1, 'youve': 1, 'gone': 1, 'insane': 1, 'previous': 1, 'aka': 1, 'typical': 1, 'bestof': 1, '4': 1, 'day': 1, 'link': 1, 'month': 1, 'complains': 1, 'bot': 1, 'downvotes': 1, 'etc': 1, 'insta': 1, 'upvotes': 1, 'frontpage': 1, 'kinda': 1, 'obvious': 1, 'exactly': 1, 'spread': 1, 'misinformation': 1, 'narrative': 1, 'much': 1, 'utorrent': 1, '2017': 1, 'lul': 1, 'vote': 1, 'wallet': 1, 'vac': 1, 'httpsenwikipediaorgwikiembraceextendandextinguish': 1, 'try': 1, 'wider': 1, '1920': 1, 'horizontal': 1, 'scrolling': 1, 'cyanogen': 1, 'another': 1, 'httpiimgurcomgzpymfjjpg': 1, 'rcringe': 1})\n",
      "[('one', 7), ('terrorist', 7), ('get', 6), ('make', 5), ('like', 5), ('people', 5), ('american', 4), ('wa', 4), ('dont', 4), ('bitcoin', 4)]\n"
     ]
    }
   ],
   "source": [
    "'''Analyze upvoted comments'''\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "stopwords = stopwords.words('english') #Insignificant word filter\n",
    "\n",
    "#aiming to remove inflectional endings only and to return the base or dictionary form of a word\n",
    "lemmatizer = WordNetLemmatizer() #Lemmatizer instance\n",
    "\n",
    "upvoted_comments = dataframe_comments.query('ups >= 50').copy() #Gather comments with (50) or more upvotes\n",
    "tokenized_comments = [] #Tokenized comments\n",
    "\n",
    "#Tokenize each comment\n",
    "for itr in upvoted_comments.index.values:\n",
    "    lowercased = upvoted_comments['body'][itr].lower() #Lowercase all strings\n",
    "    #Strip punctuation\n",
    "    for punct in string.punctuation:\n",
    "        lowercased = lowercased.replace(punct, \"\")\n",
    "    tokenized_comments.append(nltk.word_tokenize(lowercased, language = 'English'))\n",
    "\n",
    "#Lemmatize each word\n",
    "for index in range(0, len(tokenized_comments)):\n",
    "    for w in range(0, len(tokenized_comments[index])):\n",
    "        tokenized_comments[index][w] = lemmatizer.lemmatize(tokenized_comments[index][w])\n",
    "        \n",
    "#Filter out insignificant words\n",
    "for i in range(0, len(tokenized_comments)):\n",
    "    tokenized_comments[i] = [word for word in tokenized_comments[i] if word not in stopwords]\n",
    "\n",
    "#Find counts of all words\n",
    "c = Counter()\n",
    "for lyst in tokenized_comments:\n",
    "    for werd in lyst:\n",
    "        c[werd] += 1\n",
    "    \n",
    "print(c)\n",
    "print(c.most_common(10))\n",
    "\n",
    "'''\n",
    "Need to: compare most common words for all comments and downvoted comments.\n",
    "Find Subreddits that posts were most popular in\n",
    "Possibly some sort of clustering\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_mean =  0.175505438831769 neu_mean =  0.7289067203099368 neg_mean =  0.08977484726568317 compound_mean =  0.11452387125614642\n",
      "pos_mean - neg_mean =  0.08573059156608583\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullname</th>\n",
       "      <th>archived</th>\n",
       "      <th>author.name</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>downs</th>\n",
       "      <th>edited</th>\n",
       "      <th>...</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit.display_name_prefixed</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>ups</th>\n",
       "      <th>polarity_score</th>\n",
       "      <th>polarity_score_positive</th>\n",
       "      <th>polarity_score_neutral</th>\n",
       "      <th>polarity_score_negative</th>\n",
       "      <th>polarity_score_compound</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t1_d687zh5</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A hard look at training and tactics\" = They wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.470604e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Bad_Cop_No_Donut</td>\n",
       "      <td>public</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.097, 'neu': 0.903, 'pos': 0.0, 'comp...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t1_d5wqzhx</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>They deserve all of the hate</td>\n",
       "      <td>0</td>\n",
       "      <td>1.469847e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Bad_Cop_No_Donut</td>\n",
       "      <td>public</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.425, 'neu': 0.575, 'pos': 0.0, 'comp...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.425</td>\n",
       "      <td>-0.5719</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t1_d5qvqfw</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I guess that's what they mean when say \"I don'...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.469498e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Bad_Cop_No_Donut</td>\n",
       "      <td>public</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t1_d5quz9y</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It's never too late for them, It's never too c...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.469497e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Bad_Cop_No_Donut</td>\n",
       "      <td>public</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.151, 'neu': 0.736, 'pos': 0.113, 'co...</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.2565</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t1_d565ls1</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://petitions.whitehouse.gov//petition/pet...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.468114e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Good_Cop_Free_Donut</td>\n",
       "      <td>public</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     fullname  archived  author.name author_flair_text  \\\n",
       "0  t1_d687zh5      True  BlackToLive               NaN   \n",
       "1  t1_d5wqzhx      True  BlackToLive               NaN   \n",
       "2  t1_d5qvqfw      True  BlackToLive               NaN   \n",
       "3  t1_d5quz9y      True  BlackToLive               NaN   \n",
       "4  t1_d565ls1      True  BlackToLive               NaN   \n",
       "\n",
       "                                                body  controversiality  \\\n",
       "0  A hard look at training and tactics\" = They wi...                 0   \n",
       "1                       They deserve all of the hate                 0   \n",
       "2  I guess that's what they mean when say \"I don'...                 0   \n",
       "3  It's never too late for them, It's never too c...                 0   \n",
       "4  https://petitions.whitehouse.gov//petition/pet...                 0   \n",
       "\n",
       "    created_utc  distinguished  downs edited  ...   stickied  \\\n",
       "0  1.470604e+09            NaN      0  False  ...      False   \n",
       "1  1.469847e+09            NaN      0  False  ...      False   \n",
       "2  1.469498e+09            NaN      0  False  ...      False   \n",
       "3  1.469497e+09            NaN      0  False  ...      False   \n",
       "4  1.468114e+09            NaN      0  False  ...      False   \n",
       "\n",
       "  subreddit.display_name_prefixed  subreddit_type  ups  \\\n",
       "0              r/Bad_Cop_No_Donut          public    1   \n",
       "1              r/Bad_Cop_No_Donut          public    1   \n",
       "2              r/Bad_Cop_No_Donut          public    1   \n",
       "3              r/Bad_Cop_No_Donut          public    1   \n",
       "4           r/Good_Cop_Free_Donut          public    1   \n",
       "\n",
       "                                      polarity_score polarity_score_positive  \\\n",
       "0  {'neg': 0.097, 'neu': 0.903, 'pos': 0.0, 'comp...                   0.000   \n",
       "1  {'neg': 0.425, 'neu': 0.575, 'pos': 0.0, 'comp...                   0.000   \n",
       "2  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...                   0.000   \n",
       "3  {'neg': 0.151, 'neu': 0.736, 'pos': 0.113, 'co...                   0.113   \n",
       "4  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...                   0.000   \n",
       "\n",
       "  polarity_score_neutral polarity_score_negative polarity_score_compound label  \n",
       "0                  0.903                   0.097                 -0.1027     0  \n",
       "1                  0.575                   0.425                 -0.5719    -1  \n",
       "2                  1.000                   0.000                  0.0000     0  \n",
       "3                  0.736                   0.151                 -0.2565    -1  \n",
       "4                  1.000                   0.000                  0.0000     0  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sentiment Analysis of Comments Using Vader\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk import tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "from collections import Counter\n",
    "import string\n",
    "from pprint import pprint\n",
    "\n",
    "sia = SIA()\n",
    "comments_list = []\n",
    "for itr in dataframe_comments.index.values:\n",
    "    comments_list.append(dataframe_comments['body'][itr])\n",
    "\n",
    "positive_results = []\n",
    "neutral_results = []\n",
    "negative_results = []\n",
    "compound_results = []\n",
    "for itr in comments_list:\n",
    "    pol_score = sia.polarity_scores(itr)\n",
    "    positive_results.append(pol_score['pos'])\n",
    "    neutral_results.append(pol_score['neu'])\n",
    "    negative_results.append(pol_score['neg'])\n",
    "    compound_results.append(pol_score['compound'])\n",
    "    #results.append(pol_score)\n",
    "\n",
    "#df1 = DataFrame({'polarity_score': results})\n",
    "df1 = pd.DataFrame({'polarity_score_positive': positive_results})\n",
    "df2 = pd.DataFrame({'polarity_score_neutral': neutral_results})\n",
    "df3 = pd.DataFrame({'polarity_score_negative': negative_results})\n",
    "df4 = pd.DataFrame({'polarity_score_compound': compound_results})\n",
    "\n",
    "#Added Polarity Scores to dataframe_comments\n",
    "dataframe_comments['polarity_score_positive'] = df1\n",
    "dataframe_comments['polarity_score_neutral'] = df2\n",
    "dataframe_comments['polarity_score_negative'] = df3\n",
    "dataframe_comments['polarity_score_compound'] = df4\n",
    "\n",
    "pos_mean = dataframe_comments['polarity_score_positive'].mean()\n",
    "neu_mean = dataframe_comments['polarity_score_neutral'].mean()\n",
    "neg_mean = dataframe_comments['polarity_score_negative'].mean()\n",
    "compound_mean = dataframe_comments['polarity_score_compound'].mean()\n",
    "\n",
    "print('pos_mean = ', pos_mean, 'neu_mean = ', neu_mean, 'neg_mean = ', neg_mean, 'compound_mean = ', compound_mean) \n",
    "print('pos_mean - neg_mean = ', pos_mean - neg_mean)\n",
    "dataframe_comments.head()\n",
    "#print(dataframe_comments['polarity_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfYAAAHWCAYAAACFR6uKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e986516908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Graph Pos, Neu, Neg Polarity Scores\n",
    "import seaborn as sns\n",
    "sns.set(style='darkgrid', context='talk', palette='Dark2')\n",
    "\n",
    "#Add Labels for Neg, Pos, and Neutral Polarity Scores\n",
    "dataframe_comments['label'] = 0\n",
    "dataframe_comments.loc[dataframe_comments['polarity_score_compound'] > 0.2, 'label'] = 1\n",
    "dataframe_comments.loc[dataframe_comments['polarity_score_compound'] < -0.2, 'label'] = -1\n",
    "dataframe_comments.head()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "counts = dataframe_comments.label.value_counts(normalize=True) * 100\n",
    "\n",
    "sns.barplot(x=counts.index, y=counts, ax=ax)\n",
    "\n",
    "ax.set_xticklabels(['Negative', 'Neutral', 'Positive'])\n",
    "ax.set_ylabel(\"Percentage\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('a', 'lot', 'of'), 62), (('one', 'of', 'the'), 55), (('ethan', 'bradberry', 'ethan'), 54), (('bradberry', 'ethan', 'bradberry'), 54), (('f', 'u', 'c'), 40), (('u', 'c', 'k'), 40), (('c', 'k', 'm'), 40), (('k', 'm', 'u'), 40), (('m', 'u', 's'), 40), (('u', 's', 'l'), 40), (('s', 'l', 'i'), 40), (('l', 'i', 'm'), 40), (('i', 'm', 's'), 40), (('retarded', 'faggot', 'retarded'), 40), (('faggot', 'retarded', 'faggot'), 40), (('m', 's', 'f'), 39), (('s', 'f', 'u'), 39), (('agree', 'with', 'you'), 38), (('it', 'â€™', 's'), 36), (('i', 'am', 'not'), 35), (('i', 'agree', 'with'), 34), (('this', 'is', 'a'), 31), (('this', 'is', 'the'), 29), (('there', 'is', 'no'), 28), (('i', 'dont', 'think'), 27), (('there', 'is', 'a'), 25), (('i', 'do', 'not'), 24), (('do', 'you', 'think'), 23), (('is', 'going', 'to'), 23), (('it', 'will', 'be'), 22), (('i', 'want', 'to'), 22), (('i', 'think', 'that'), 22), (('is', 'not', 'a'), 21), (('it', 'wa', 'a'), 21), (('i', 'think', 'it'), 21), (('is', 'one', 'of'), 20), (('it', 'would', 'be'), 20), (('going', 'to', 'be'), 20), (('i', 'â€™', 'm'), 20), (('i', 'dont', 'know'), 19), (('i', 'like', 'the'), 19), (('are', 'going', 'to'), 18), (('to', 'be', 'a'), 18), (('need', 'to', 'be'), 18), (('in', 'the', 'world'), 18), (('lot', 'of', 'people'), 18), (('it', 'look', 'like'), 18), (('what', 'do', 'you'), 18), (('by', 'the', 'way'), 18), (('if', 'you', 'want'), 18), (('you', 'want', 'to'), 18), (('out', 'of', 'the'), 17), (('if', 'you', 'have'), 17), (('to', 'be', 'the'), 17), (('be', 'able', 'to'), 17), (('don', 'â€™', 't'), 17), (('the', 'end', 'of'), 16), (('seems', 'to', 'be'), 16), (('the', 'fact', 'that'), 16), (('the', 'same', 'time'), 16), (('the', 'rest', 'of'), 16), (('this', 'is', 'not'), 16), (('you', 'have', 'to'), 16), (('it', 'is', 'a'), 16), (('i', 'would', 'like'), 15), (('most', 'of', 'the'), 15), (('a', 'couple', 'of'), 15), (('when', 'i', 'wa'), 15), (('it', 'just', 'a'), 15), (('know', 'how', 'to'), 14), (('would', 'like', 'to'), 14), (('thanks', 'for', 'the'), 14), (('is', 'not', 'the'), 14), (('i', 'would', 'recommend'), 14), (('if', 'you', 'are'), 13), (('will', 'be', 'the'), 13), (('thanks', 'for', 'posting'), 13), (('at', 'the', 'same'), 13), (('thank', 'you', 'for'), 13), (('of', 'the', 'most'), 13), (('part', 'of', 'the'), 13), (('in', 'the', 'first'), 13), (('is', 'the', 'best'), 13), (('salt', 'and', 'tie'), 13), (('go', 'to', 'the'), 13), (('we', 'have', 'to'), 12), (('a', 'little', 'bit'), 12), (('first', 'of', 'all'), 12), (('we', 'need', 'to'), 12), (('the', 'first', 'place'), 12), (('of', 'the', 'world'), 12), (('it', 'can', 'be'), 12), (('in', 'the', 'middle'), 11), (('a', 'long', 'time'), 11), (('that', 'would', 'be'), 11), (('dont', 'want', 'to'), 11), (('that', 'â€™', 's'), 11), (('it', 'is', 'not'), 11), (('and', 'i', 'dont'), 11), (('to', 'invest', 'in'), 11)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesse\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    }
   ],
   "source": [
    "'''Seach for Common Phrases in Comments Using Ngrams'''\n",
    "\n",
    "stopwords = stopwords.words('english') #Insignificant word filter\n",
    "\n",
    "#aiming to remove inflectional endings only and to return the base or dictionary form of a word\n",
    "lemmatizer = WordNetLemmatizer() #Lemmatizer instance\n",
    "\n",
    "#All Comments Tokenized\n",
    "tokenized_comments_all = []\n",
    "\n",
    "#Tokenize each comment\n",
    "for itr in dataframe_comments.index.values:\n",
    "    lowercased = dataframe_comments['body'][itr].lower() #Lowercase all strings\n",
    "    #Strip punctuation\n",
    "    for punct in string.punctuation:\n",
    "        lowercased = lowercased.replace(punct, \"\")\n",
    "    tokenized_comments_all.append(nltk.word_tokenize(lowercased, language = 'English'))\n",
    "\n",
    "#Lemmatize each word\n",
    "for index in range(0, len(tokenized_comments_all)):\n",
    "    for w in range(0, len(tokenized_comments_all[index])):\n",
    "        tokenized_comments_all[index][w] = lemmatizer.lemmatize(tokenized_comments_all[index][w])\n",
    "\n",
    "\n",
    "#Create n-grams of num_tokens_to_gram tokens\n",
    "num_tokens_to_gram = 3\n",
    "\n",
    "n_gram_tokens_all = []\n",
    "\n",
    "for num in range(0, len(tokenized_comments_all)):\n",
    "    n_gram_tokens = ngrams(tokenized_comments_all[num], num_tokens_to_gram)\n",
    "    n_gram_tokens_all.append(n_gram_tokens)\n",
    "#     if num < 10:\n",
    "#        print(Counter(n_gram_tokens))\n",
    "\n",
    "#Find counts of identical tokens\n",
    "c = Counter()\n",
    "for lyst in n_gram_tokens_all:\n",
    "    for gram in lyst:\n",
    "        c[gram] += 1\n",
    "    \n",
    "#print(c)\n",
    "print(c.most_common(100))\n",
    "\n",
    "# print(n_gram_3_tokens_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('retarded', 'faggot', 'retarded', 'faggot'), 40), (('faggot', 'retarded', 'faggot', 'retarded'), 39), (('in', 'the', 'first', 'place'), 7), (('i', 'agree', 'with', 'you'), 7), (('i', 'am', 'not', 'a'), 6), (('at', 'the', 'same', 'time'), 5), (('i', 'am', 'not', 'sure'), 5), (('is', 'not', 'the', 'same'), 4), (('like', 'it', 'or', 'not'), 4), (('a', 'lot', 'of', 'people'), 4), (('did', 'a', 'call', 'for'), 4), (('dysfunctional', 'family', 'of', 'origin'), 4), (('family', 'of', 'origin', 'not'), 4), (('am', 'not', 'a', 'bot'), 4), (('couldnt', 'care', 'le', 'about'), 3), (('in', 'the', 'middle', 'of'), 3), (('if', 'you', 'have', 'any'), 3), (('for', 'the', 'rest', 'of'), 3), (('im', 'gon', 'na', 'die'), 3), (('on', 'the', 'other', 'hand'), 3), (('when', 'it', 'come', 'to'), 3), (('the', 'only', 'difference', 'is'), 3), (('from', 'time', 'to', 'time'), 3), (('country', 'in', 'the', 'world'), 3), (('especially', 'if', 'he', 'is'), 3), (('i', 'do', 'not', 'care'), 3), (('the', 'cop', 'should', 'be'), 3), (('37yearold', 'alton', 'sterling', 'a'), 3), (('alton', 'sterling', 'a', 'father'), 3), (('sterling', 'a', 'father', 'of'), 3), (('a', 'father', 'of', 'five'), 3), (('cop', 'pulled', 'his', 'gun'), 3), (('pulled', 'his', 'gun', 'pointed'), 3), (('his', 'gun', 'pointed', 'it'), 3), (('gun', 'pointed', 'it', 'on'), 3), (('pointed', 'it', 'on', 'the'), 3), (('it', 'on', 'the', 'man'), 3), (('on', 'the', 'man', 'â€™'), 3), (('the', 'man', 'â€™', 's'), 3), (('man', 'â€™', 's', 'chest'), 3), (('â€™', 's', 'chest', 'and'), 3), (('s', 'chest', 'and', 'fired'), 3), (('chest', 'and', 'fired', 'multiple'), 3), (('and', 'fired', 'multiple', 'shot'), 3), (('the', 'incident', 'said', 'that'), 3), (('incident', 'said', 'that', 'sterling'), 3), (('said', 'that', 'sterling', 'wasn'), 3), (('that', 'sterling', 'wasn', 'â€™'), 3), (('sterling', 'wasn', 'â€™', 't'), 3), (('wasn', 'â€™', 't', 'holding'), 3), (('â€™', 't', 'holding', 'a'), 3), (('t', 'holding', 'a', 'weapon'), 3), (('whats', 'the', 'point', 'of'), 2), (('the', 'middle', 'of', 'a'), 2), (('it', 'my', 'feeling', 'with'), 2), (('my', 'feeling', 'with', 'this'), 2), (('feeling', 'with', 'this', 'and'), 2), (('with', 'this', 'and', 'the'), 2), (('this', 'and', 'the', 'slow'), 2), (('and', 'the', 'slow', 'death'), 2), (('the', 'slow', 'death', 'of'), 2), (('slow', 'death', 'of', 'the'), 2), (('death', 'of', 'the', 'twitter'), 2), (('of', 'the', 'twitter', 'war'), 2), (('the', 'twitter', 'war', 'that'), 2), (('twitter', 'war', 'that', 'these'), 2), (('war', 'that', 'these', 'bozo'), 2), (('that', 'these', 'bozo', 'have'), 2), (('these', 'bozo', 'have', 'overplayed'), 2), (('bozo', 'have', 'overplayed', 'their'), 2), (('have', 'overplayed', 'their', 'hand'), 2), (('overplayed', 'their', 'hand', 'sell'), 2), (('their', 'hand', 'sell', 'your'), 2), (('hand', 'sell', 'your', 'zcash'), 2), (('sell', 'your', 'zcash', 'enigma'), 2), (('your', 'zcash', 'enigma', 'and'), 2), (('zcash', 'enigma', 'and', 'spectre'), 2), (('enigma', 'and', 'spectre', 'now'), 2), (('and', 'spectre', 'now', 'if'), 2), (('spectre', 'now', 'if', 'you'), 2), (('now', 'if', 'you', 'have'), 2), (('you', 'have', 'any', 'before'), 2), (('have', 'any', 'before', 'these'), 2), (('any', 'before', 'these', 'coin'), 2), (('before', 'these', 'coin', 'crater'), 2), (('in', 'the', 'case', 'of'), 2), (('shot', 'in', 'the', 'head'), 2), (('a', 'lot', 'of', 'way'), 2), (('one', 'hell', 'of', 'a'), 2), (('he', 'â€™', 's', 'doing'), 2), (('what', 'the', 'hell', 'is'), 2), (('one', 'of', 'the', 'most'), 2), (('it', 'odd', 'to', 'have'), 2), (('odd', 'to', 'have', 'the'), 2), (('to', 'have', 'the', 'prosecution'), 2), (('have', 'the', 'prosecution', 'being'), 2), (('the', 'prosecution', 'being', 'done'), 2), (('prosecution', 'being', 'done', 'by'), 2), (('being', 'done', 'by', 'a'), 2), (('done', 'by', 'a', 'country'), 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesse\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:53: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    }
   ],
   "source": [
    "'''Seach for Common Phrases in Comments Using Ngrams - Negative Polarity Score'''\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk import tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "from collections import Counter\n",
    "import string\n",
    "from pprint import pprint\n",
    "\n",
    "stopwords = stopwords.words('english') #Insignificant word filter\n",
    "\n",
    "#aiming to remove inflectional endings only and to return the base or dictionary form of a word\n",
    "lemmatizer = WordNetLemmatizer() #Lemmatizer instance\n",
    "\n",
    "#All Comments Tokenized\n",
    "#controversial_comments = dataframe_comments.query('controversiality > 0').copy()\n",
    "comments_neg_polarity_score = dataframe_comments.query('label == -1').copy()\n",
    "\n",
    "tokenized_comments_neg_polarity_score = []\n",
    "\n",
    "#Tokenize each comment\n",
    "for itr in comments_neg_polarity_score.index.values:\n",
    "#     print(itr)\n",
    "#     print(comments_neg_polarity_score['body'][itr])\n",
    "    lowercased = comments_neg_polarity_score['body'][itr].lower() #Lowercase all strings\n",
    "    #Strip punctuation\n",
    "    for punct in string.punctuation:\n",
    "        lowercased = lowercased.replace(punct, \"\")\n",
    "    tokenized_comments_neg_polarity_score.append(nltk.word_tokenize(lowercased, language = 'English'))\n",
    "\n",
    "#Lemmatize each word\n",
    "for index in range(0, len(tokenized_comments_neg_polarity_score)):\n",
    "    for w in range(0, len(tokenized_comments_neg_polarity_score[index])):\n",
    "        tokenized_comments_neg_polarity_score[index][w] = lemmatizer.lemmatize(tokenized_comments_neg_polarity_score[index][w])\n",
    "\n",
    "\n",
    "#Create n-grams of num_tokens_to_gram tokens\n",
    "num_tokens_to_gram = 4\n",
    "\n",
    "n_gram_tokens_all = []\n",
    "\n",
    "for num in range(0, len(tokenized_comments_neg_polarity_score)):\n",
    "    n_gram_tokens = ngrams(tokenized_comments_neg_polarity_score[num], num_tokens_to_gram)\n",
    "    n_gram_tokens_all.append(n_gram_tokens)\n",
    "#     if num < 10:\n",
    "#        print(Counter(n_gram_tokens))\n",
    "\n",
    "#Find counts of identical tokens\n",
    "c = Counter()\n",
    "for lyst in n_gram_tokens_all:\n",
    "    for gram in lyst:\n",
    "        c[gram] += 1\n",
    "    \n",
    "#print(c)\n",
    "print(c.most_common(100))\n",
    "\n",
    "# print(n_gram_3_tokens_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('i', 'agree', 'with', 'you'), 19), (('is', 'one', 'of', 'the'), 11), (('a', 'close', 'eye', 'on'), 9), (('the', 'end', 'of', 'the'), 8), (('i', 'would', 'like', 'to'), 8), (('i', 'would', 'recommend', 'to'), 8), (('keep', 'a', 'close', 'eye'), 8), (('there', 'is', 'a', 'point'), 8), (('is', 'a', 'point', 'to'), 8), (('i', 'like', 'the', 'idea'), 8), (('one', 'of', 'the', 'best'), 7), (('one', 'of', 'the', 'most'), 7), (('go', 'to', 'the', 'moon'), 7), (('what', 'do', 'you', 'think'), 6), (('recommend', 'to', 'keep', 'a'), 6), (('to', 'keep', 'a', 'close'), 6), (('pay', 'more', 'attention', 'to'), 6), (('one', 'of', 'them', 'will'), 6), (('a', 'lot', 'of', 'people'), 6), (('thanks', 'for', 'posting', 'this'), 5), (('at', 'the', 'same', 'time'), 5), (('do', 'your', 'own', 'research'), 5), (('the', 'rest', 'of', 'the'), 5), (('would', 'recommend', 'to', 'keep'), 5), (('to', 'pay', 'more', 'attention'), 5), (('of', 'them', 'will', 'go'), 5), (('them', 'will', 'go', 'to'), 5), (('will', 'go', 'to', 'the'), 5), (('idea', 'underlying', 'the', 'project'), 5), (('if', 'you', 'want', 'to'), 5), (('listed', 'in', 'the', 'post'), 5), (('salt', 'storm', 'and', 'tie'), 5), (('randomemailaddressgmailcom', 'randomemailaddressgmailcom', 'randomemailaddressgmailcom', 'randomemailaddressgmailcom'), 5), (('a', 'for', 'me', 'i'), 4), (('and', 'there', 'is', 'no'), 4), (('i', 'dont', 'want', 'to'), 4), (('like', 'the', 'idea', 'underlying'), 4), (('the', 'idea', 'underlying', 'the'), 4), (('i', 'think', 'this', 'is'), 4), (('like', 'salt', 'storm', 'and'), 4), (('this', 'is', 'one', 'of'), 3), (('you', 'are', 'going', 'to'), 3), (('might', 'be', 'a', 'good'), 3), (('who', 'are', 'interested', 'in'), 3), (('and', 'i', 'dont', 'think'), 3), (('more', 'attention', 'to', 'the'), 3), (('one', 'of', 'the', 'biggest'), 3), (('most', 'of', 'the', 'time'), 3), (('the', 'best', 'way', 'to'), 3), (('should', 'be', 'free', 'for'), 3), (('in', 'the', 'first', 'place'), 3), (('on', 'the', 'other', 'hand'), 3), (('way', 'of', 'looking', 'at'), 3), (('of', 'looking', 'at', 'the'), 3), (('looking', 'at', 'the', 'world'), 3), (('is', 'the', 'best', 'thing'), 3), (('to', 'take', 'care', 'of'), 3), (('rest', 'of', 'the', 'world'), 3), (('by', 'the', 'way', 'i'), 3), (('but', 'i', 'dont', 'think'), 3), (('i', 'want', 'to', 'know'), 3), (('what', 'is', 'more', 'comfortable'), 3), (('is', 'more', 'comfortable', 'for'), 3), (('would', 'be', 'better', 'if'), 3), (('it', 'is', 'necessary', 'to'), 3), (('necessary', 'to', 'admit', 'that'), 3), (('it', 'is', 'impossible', 'to'), 3), (('is', 'impossible', 'to', 'predict'), 3), (('surprised', 'to', 'see', 'tie'), 3), (('to', 'see', 'tie', 'here'), 3), (('i', 'â€™', 'm', 'not'), 3), (('ha', 'nothing', 'to', 'do'), 3), (('nothing', 'to', 'do', 'with'), 3), (('agree', 'with', 'you', 'that'), 3), (('this', 'is', 'just', 'a'), 3), (('it', 'is', 'important', 'to'), 3), (('this', 'form', 'of', 't'), 3), (('at', 'the', 'end', 'of'), 3), (('end', 'of', 'the', 'day'), 3), (('barely', 'known', 'token', 'that'), 3), (('token', 'that', 'might', 'go'), 3), (('that', 'might', 'go', 'off'), 3), (('a', 'point', 'to', 'invest'), 3), (('point', 'to', 'invest', 'in'), 3), (('to', 'invest', 'in', 'token'), 3), (('invest', 'in', 'token', 'barely'), 3), (('in', 'token', 'barely', 'known'), 3), (('token', 'barely', 'known', 'to'), 3), (('barely', 'known', 'to', 'anyone'), 3), (('known', 'to', 'anyone', 'but'), 3), (('to', 'anyone', 'but', 'having'), 3), (('anyone', 'but', 'having', 'great'), 3), (('but', 'having', 'great', 'potential'), 3), (('having', 'great', 'potential', 'choose'), 3), (('great', 'potential', 'choose', 'five'), 3), (('potential', 'choose', 'five', 'or'), 3), (('choose', 'five', 'or', 'six'), 3), (('five', 'or', 'six', 'and'), 3), (('or', 'six', 'and', 'one'), 3), (('six', 'and', 'one', 'of'), 3)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesse\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:51: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    }
   ],
   "source": [
    "'''Seach for Common Phrases in Comments Using Ngrams - Positive Polarity Score'''\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk import tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "from collections import Counter\n",
    "import string\n",
    "from pprint import pprint\n",
    "\n",
    "stopwords = stopwords.words('english') #Insignificant word filter\n",
    "\n",
    "#aiming to remove inflectional endings only and to return the base or dictionary form of a word\n",
    "lemmatizer = WordNetLemmatizer() #Lemmatizer instance\n",
    "\n",
    "#All Comments Tokenized\n",
    "#controversial_comments = dataframe_comments.query('controversiality > 0').copy()\n",
    "comments_pos_polarity_score = dataframe_comments.query('label == 1').copy()\n",
    "\n",
    "tokenized_comments_pos_polarity_score = []\n",
    "\n",
    "#Tokenize each comment\n",
    "for itr in comments_pos_polarity_score.index.values:\n",
    "#     print(itr)\n",
    "#     print(comments_pos_polarity_score['body'][itr])\n",
    "    lowercased = comments_pos_polarity_score['body'][itr].lower() #Lowercase all strings\n",
    "    #Strip punctuation\n",
    "    for punct in string.punctuation:\n",
    "        lowercased = lowercased.replace(punct, \"\")\n",
    "    tokenized_comments_pos_polarity_score.append(nltk.word_tokenize(lowercased, language = 'English'))\n",
    "\n",
    "#Lemmatize each word\n",
    "for index in range(0, len(tokenized_comments_pos_polarity_score)):\n",
    "    for w in range(0, len(tokenized_comments_pos_polarity_score[index])):\n",
    "        tokenized_comments_pos_polarity_score[index][w] = lemmatizer.lemmatize(tokenized_comments_pos_polarity_score[index][w])\n",
    "\n",
    "\n",
    "#Create n-grams of num_tokens_to_gram tokens\n",
    "num_tokens_to_gram = 4\n",
    "\n",
    "n_gram_tokens_all = []\n",
    "\n",
    "for num in range(0, len(tokenized_comments_pos_polarity_score)):\n",
    "    n_gram_tokens = ngrams(tokenized_comments_pos_polarity_score[num], num_tokens_to_gram)\n",
    "    n_gram_tokens_all.append(n_gram_tokens)\n",
    "\n",
    "#Find counts of identical tokens\n",
    "c = Counter()\n",
    "for lyst in n_gram_tokens_all:\n",
    "    for gram in lyst:\n",
    "        c[gram] += 1\n",
    "\n",
    "#print(c)\n",
    "print(c.most_common(100))\n",
    "#print(n_gram_3_tokens_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('ethan', 'bradberry', 'ethan', 'bradberry'), 54), (('bradberry', 'ethan', 'bradberry', 'ethan'), 53), (('f', 'u', 'c', 'k'), 40), (('u', 'c', 'k', 'm'), 40), (('c', 'k', 'm', 'u'), 40), (('k', 'm', 'u', 's'), 40), (('m', 'u', 's', 'l'), 40), (('u', 's', 'l', 'i'), 40), (('s', 'l', 'i', 'm'), 40), (('l', 'i', 'm', 's'), 40), (('i', 'm', 's', 'f'), 39), (('m', 's', 'f', 'u'), 39), (('s', 'f', 'u', 'c'), 39), (('ay', 'yo', 'le', 'mao'), 5), (('for', 'a', 'long', 'time'), 4), (('a', 'lot', 'of', 'people'), 3), (('the', 'rest', 'of', 'the'), 3), (('is', 'going', 'to', 'be'), 3), (('a', 'long', 'a', 'the'), 3), (('ha', 'nothing', 'to', 'do'), 3), (('nothing', 'to', 'do', 'with'), 3), (('is', 'one', 'of', 'the'), 3), (('am', 'i', 'the', 'only'), 3), (('at', 'the', 'same', 'time'), 3), (('i', 'know', 'that', 'feel'), 3), (('a', 'normal', 'day', 'in'), 3), (('this', 'wa', 'a', 'very'), 2), (('wa', 'a', 'very', 'well'), 2), (('a', 'very', 'well', 'directed'), 2), (('very', 'well', 'directed', 'fud'), 2), (('well', 'directed', 'fud', 'attack'), 2), (('directed', 'fud', 'attack', 'with'), 2), (('fud', 'attack', 'with', 'lot'), 2), (('attack', 'with', 'lot', 'of'), 2), (('with', 'lot', 'of', 'interested'), 2), (('lot', 'of', 'interested', 'party'), 2), (('of', 'interested', 'party', 'the'), 2), (('interested', 'party', 'the', 'people'), 2), (('party', 'the', 'people', 'involved'), 2), (('the', 'people', 'involved', 'should'), 2), (('people', 'involved', 'should', 'be'), 2), (('involved', 'should', 'be', 'barred'), 2), (('should', 'be', 'barred', 'from'), 2), (('be', 'barred', 'from', 'journalism'), 2), (('barred', 'from', 'journalism', 'and'), 2), (('from', 'journalism', 'and', 'academia'), 2), (('xlm', 'tie', 'and', 'ada'), 2), (('tie', 'and', 'ada', 'hodl'), 2), (('and', 'ada', 'hodl', 'or'), 2), (('ada', 'hodl', 'or', 'not'), 2), (('hodl', 'or', 'not', 'to'), 2), (('or', 'not', 'to', 'hodl'), 2), (('not', 'to', 'hodl', 'that'), 2), (('to', 'hodl', 'that', 'is'), 2), (('hodl', 'that', 'is', 'the'), 2), (('that', 'is', 'the', 'question'), 2), (('can', 'â€™', 't', 'wait'), 2), (('â€™', 't', 'wait', 'to'), 2), (('t', 'wait', 'to', 'start'), 2), (('don', 'â€™', 't', 'want'), 2), (('â€™', 't', 'want', 'to'), 2), (('i', 'wouldnt', 'be', 'surprised'), 2), (('i', 'want', 'to', 'be'), 2), (('when', 'i', 'wa', 'a'), 2), (('im', 'not', 'sure', 'what'), 2), (('the', 'united', 'state', 'hasn'), 2), (('united', 'state', 'hasn', 'â€™'), 2), (('state', 'hasn', 'â€™', 't'), 2), (('hasn', 'â€™', 't', 'minted'), 2), (('â€™', 't', 'minted', 'any'), 2), (('t', 'minted', 'any', 'new'), 2), (('minted', 'any', 'new', 'purple'), 2), (('any', 'new', 'purple', 'heart'), 2), (('new', 'purple', 'heart', 'medal'), 2), (('purple', 'heart', 'medal', 'since'), 2), (('heart', 'medal', 'since', 'world'), 2), (('medal', 'since', 'world', 'war'), 2), (('since', 'world', 'war', 'ii'), 2), (('world', 'war', 'ii', 'we'), 2), (('war', 'ii', 'we', 'â€™'), 2), (('ii', 'we', 'â€™', 've'), 2), (('we', 'â€™', 've', 'been'), 2), (('â€™', 've', 'been', 'using'), 2), (('ve', 'been', 'using', 'the'), 2), (('been', 'using', 'the', 'stockpile'), 2), (('using', 'the', 'stockpile', 'that'), 2), (('the', 'stockpile', 'that', 'wa'), 2), (('stockpile', 'that', 'wa', 'prepared'), 2), (('that', 'wa', 'prepared', 'in'), 2), (('wa', 'prepared', 'in', 'anticipation'), 2), (('prepared', 'in', 'anticipation', 'of'), 2), (('in', 'anticipation', 'of', 'a'), 2), (('anticipation', 'of', 'a', 'ground'), 2), (('of', 'a', 'ground', 'invasion'), 2), (('a', 'ground', 'invasion', 'of'), 2), (('ground', 'invasion', 'of', 'japan'), 2), (('o', 'dab', 'o', 'dab'), 2), (('dab', 'o', 'dab', 'o'), 2), (('be', 'one', 'of', 'them'), 2), (('do', 'i', 'have', 'to'), 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesse\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:51: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    }
   ],
   "source": [
    "'''Seach for Common Phrases in Comments Using Ngrams - Neutral Polarity Score'''\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk import tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "from collections import Counter\n",
    "import string\n",
    "from pprint import pprint\n",
    "\n",
    "stopwords = stopwords.words('english') #Insignificant word filter\n",
    "\n",
    "#aiming to remove inflectional endings only and to return the base or dictionary form of a word\n",
    "lemmatizer = WordNetLemmatizer() #Lemmatizer instance\n",
    "\n",
    "#All Comments Tokenized\n",
    "#controversial_comments = dataframe_comments.query('controversiality > 0').copy()\n",
    "comments_pos_polarity_score = dataframe_comments.query('label == 0').copy()\n",
    "\n",
    "tokenized_comments_pos_polarity_score = []\n",
    "\n",
    "#Tokenize each comment\n",
    "for itr in comments_pos_polarity_score.index.values:\n",
    "#     print(itr)\n",
    "#     print(comments_pos_polarity_score['body'][itr])\n",
    "    lowercased = comments_pos_polarity_score['body'][itr].lower() #Lowercase all strings\n",
    "    #Strip punctuation\n",
    "    for punct in string.punctuation:\n",
    "        lowercased = lowercased.replace(punct, \"\")\n",
    "    tokenized_comments_pos_polarity_score.append(nltk.word_tokenize(lowercased, language = 'English'))\n",
    "\n",
    "#Lemmatize each word\n",
    "for index in range(0, len(tokenized_comments_pos_polarity_score)):\n",
    "    for w in range(0, len(tokenized_comments_pos_polarity_score[index])):\n",
    "        tokenized_comments_pos_polarity_score[index][w] = lemmatizer.lemmatize(tokenized_comments_pos_polarity_score[index][w])\n",
    "\n",
    "\n",
    "#Create n-grams of num_tokens_to_gram tokens\n",
    "num_tokens_to_gram = 4\n",
    "\n",
    "n_gram_tokens_all = []\n",
    "\n",
    "for num in range(0, len(tokenized_comments_pos_polarity_score)):\n",
    "    n_gram_tokens = ngrams(tokenized_comments_pos_polarity_score[num], num_tokens_to_gram)\n",
    "    n_gram_tokens_all.append(n_gram_tokens)\n",
    "\n",
    "#Find counts of identical tokens\n",
    "c = Counter()\n",
    "for lyst in n_gram_tokens_all:\n",
    "    for gram in lyst:\n",
    "        c[gram] += 1\n",
    "\n",
    "#print(c)\n",
    "print(c.most_common(100))\n",
    "#print(n_gram_3_tokens_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
