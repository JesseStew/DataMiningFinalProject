{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "https://www.learndatasci.com/tutorials/sentiment-analysis-reddit-headlines-pythons-nltk/\n",
    "Grammar Analysis\n",
    "\n",
    "    Possibly compare to other accounts (or on English grammar dataset)\n",
    "        Try to figure out what their first language is.\n",
    "        \n",
    "    % of posts and comments at subreddit, maybe per account per subreddit\n",
    "        Where they are posting and commenting\n",
    "        \n",
    "    Some similarity between post/comment content and karma (upvotes/downvotes)\n",
    "        What posts gained traction\n",
    "        \n",
    "    If more than x upvotes, some type of analysis, similarity\n",
    "        Search for any claims of where they are from.\n",
    "        \n",
    "    Possibly use findings to try and detect unknown suspicious accounts\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A hard look at training and tactics\" = They will be sent more $$$ for \"training\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullname</th>\n",
       "      <th>archived</th>\n",
       "      <th>author.name</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>downs</th>\n",
       "      <th>edited</th>\n",
       "      <th>...</th>\n",
       "      <th>link_url</th>\n",
       "      <th>name</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>permalink</th>\n",
       "      <th>score</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit.display_name_prefixed</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>ups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t1_d687zh5</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A hard look at training and tactics\" = They wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.470604e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>http://www.reuters.com/article/us-usa-police-c...</td>\n",
       "      <td>t1_d687zh5</td>\n",
       "      <td>119</td>\n",
       "      <td>t3_4wkn7m</td>\n",
       "      <td>/r/Bad_Cop_No_Donut/comments/4wkn7m/chicago_po...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Bad_Cop_No_Donut</td>\n",
       "      <td>public</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t1_d5wqzhx</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>They deserve all of the hate</td>\n",
       "      <td>0</td>\n",
       "      <td>1.469847e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>http://reason.com/blog/2016/07/28/pine-bluff-c...</td>\n",
       "      <td>t1_d5wqzhx</td>\n",
       "      <td>96</td>\n",
       "      <td>t3_4v5xpc</td>\n",
       "      <td>/r/Bad_Cop_No_Donut/comments/4v5xpc/arkansas_p...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Bad_Cop_No_Donut</td>\n",
       "      <td>public</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t1_d5qvqfw</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I guess that's what they mean when say \"I don'...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.469498e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>http://countercurrentnews.com/2016/07/no-charg...</td>\n",
       "      <td>t1_d5qvqfw</td>\n",
       "      <td>210</td>\n",
       "      <td>t1_d5qeyrw</td>\n",
       "      <td>/r/Bad_Cop_No_Donut/comments/4uiezg/no_charges...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Bad_Cop_No_Donut</td>\n",
       "      <td>public</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t1_d5quz9y</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It's never too late for them, It's never too c...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.469497e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>http://www.opposingviews.com/i/society/police-...</td>\n",
       "      <td>t1_d5quz9y</td>\n",
       "      <td>18</td>\n",
       "      <td>t3_4uicjv</td>\n",
       "      <td>/r/Bad_Cop_No_Donut/comments/4uicjv/police_off...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Bad_Cop_No_Donut</td>\n",
       "      <td>public</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t1_d565ls1</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://petitions.whitehouse.gov//petition/pet...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.468114e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>http://www.thelibertyconservative.com/favor-po...</td>\n",
       "      <td>t1_d565ls1</td>\n",
       "      <td>12</td>\n",
       "      <td>t1_d55o1gr</td>\n",
       "      <td>/r/Good_Cop_Free_Donut/comments/4s0s3j/you_can...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Good_Cop_Free_Donut</td>\n",
       "      <td>public</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     fullname  archived  author.name author_flair_text  \\\n",
       "0  t1_d687zh5      True  BlackToLive               NaN   \n",
       "1  t1_d5wqzhx      True  BlackToLive               NaN   \n",
       "2  t1_d5qvqfw      True  BlackToLive               NaN   \n",
       "3  t1_d5quz9y      True  BlackToLive               NaN   \n",
       "4  t1_d565ls1      True  BlackToLive               NaN   \n",
       "\n",
       "                                                body  controversiality  \\\n",
       "0  A hard look at training and tactics\" = They wi...                 0   \n",
       "1                       They deserve all of the hate                 0   \n",
       "2  I guess that's what they mean when say \"I don'...                 0   \n",
       "3  It's never too late for them, It's never too c...                 0   \n",
       "4  https://petitions.whitehouse.gov//petition/pet...                 0   \n",
       "\n",
       "    created_utc  distinguished  downs edited ...  \\\n",
       "0  1.470604e+09            NaN      0  False ...   \n",
       "1  1.469847e+09            NaN      0  False ...   \n",
       "2  1.469498e+09            NaN      0  False ...   \n",
       "3  1.469497e+09            NaN      0  False ...   \n",
       "4  1.468114e+09            NaN      0  False ...   \n",
       "\n",
       "                                            link_url        name  \\\n",
       "0  http://www.reuters.com/article/us-usa-police-c...  t1_d687zh5   \n",
       "1  http://reason.com/blog/2016/07/28/pine-bluff-c...  t1_d5wqzhx   \n",
       "2  http://countercurrentnews.com/2016/07/no-charg...  t1_d5qvqfw   \n",
       "3  http://www.opposingviews.com/i/society/police-...  t1_d5quz9y   \n",
       "4  http://www.thelibertyconservative.com/favor-po...  t1_d565ls1   \n",
       "\n",
       "   num_comments   parent_id  \\\n",
       "0           119   t3_4wkn7m   \n",
       "1            96   t3_4v5xpc   \n",
       "2           210  t1_d5qeyrw   \n",
       "3            18   t3_4uicjv   \n",
       "4            12  t1_d55o1gr   \n",
       "\n",
       "                                           permalink score stickied  \\\n",
       "0  /r/Bad_Cop_No_Donut/comments/4wkn7m/chicago_po...     1    False   \n",
       "1  /r/Bad_Cop_No_Donut/comments/4v5xpc/arkansas_p...     1    False   \n",
       "2  /r/Bad_Cop_No_Donut/comments/4uiezg/no_charges...     1    False   \n",
       "3  /r/Bad_Cop_No_Donut/comments/4uicjv/police_off...     1    False   \n",
       "4  /r/Good_Cop_Free_Donut/comments/4s0s3j/you_can...     1    False   \n",
       "\n",
       "  subreddit.display_name_prefixed subreddit_type ups  \n",
       "0              r/Bad_Cop_No_Donut         public   1  \n",
       "1              r/Bad_Cop_No_Donut         public   1  \n",
       "2              r/Bad_Cop_No_Donut         public   1  \n",
       "3              r/Bad_Cop_No_Donut         public   1  \n",
       "4           r/Good_Cop_Free_Donut         public   1  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "dataframe_comments = pd.read_csv('reddit-suspicious-accounts/data/comments.csv', encoding='utf-8')\n",
    "dataframe_posts = pd.read_csv('reddit-suspicious-accounts/data/submissions.csv', encoding='utf-8')\n",
    "#dataframe_comments.head()\n",
    "\n",
    "#% of posts and comments at subreddit, maybe per account per subreddit\n",
    "#print(dataframe_comments['body'][0])\n",
    "dataframe_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6711\n"
     ]
    }
   ],
   "source": [
    "#Tokenizes Comments for Sentiment analysis\n",
    "tokenized_comments = []\n",
    "for itr in dataframe_comments.index.values:\n",
    "    tokenized_comments.append(nltk.word_tokenize(dataframe_comments['body'][itr], language = 'English', preserve_line = 'True'))\n",
    "\n",
    "#lc_tokenized_comments = []\n",
    "#for item in tokenized_comments:\n",
    "#    [x.lower() for x in item]\n",
    "#print(tokenized_comments[:])\n",
    "print(len(tokenized_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "#Comments Containing the words Russia or russian\n",
    "russia_reference = []\n",
    "russian_reference = []\n",
    "for itr in tokenized_comments:\n",
    "    for num in range(0, len(itr)):\n",
    "        if itr[num] == 'Russia' or itr[num] == 'russia': \n",
    "            russia_reference.append(itr)\n",
    "        if itr[num] == 'Russian' or itr[num] == 'russian':\n",
    "            russian_reference.append(itr)\n",
    "\n",
    "print(len(russia_reference))\n",
    "print(len(russian_reference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullname</th>\n",
       "      <th>archived</th>\n",
       "      <th>author.name</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>downs</th>\n",
       "      <th>edited</th>\n",
       "      <th>...</th>\n",
       "      <th>link_url</th>\n",
       "      <th>name</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>permalink</th>\n",
       "      <th>score</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit.display_name_prefixed</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>ups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>t1_d509t9i</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>By submitting to an independent, non-profit co...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.467742e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>http://blacktolive.org/?p=9589</td>\n",
       "      <td>t1_d509t9i</td>\n",
       "      <td>7</td>\n",
       "      <td>t1_d508xpp</td>\n",
       "      <td>/r/politics/comments/4rdu5x/hillarys_war_on_bl...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>r/politics</td>\n",
       "      <td>public</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>t1_crya21g</td>\n",
       "      <td>True</td>\n",
       "      <td>FaurnFlamebreaker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Well, in this country it's either you are top ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.433665e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>http://www.npr.org/2015/06/06/412314705/jury-a...</td>\n",
       "      <td>t1_crya21g</td>\n",
       "      <td>287</td>\n",
       "      <td>t3_38v84b</td>\n",
       "      <td>/r/worldnews/comments/38v84b/jury_acquits_exbp...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>r/worldnews</td>\n",
       "      <td>public</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>t1_cry9m2s</td>\n",
       "      <td>True</td>\n",
       "      <td>FaurnFlamebreaker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&gt; large-scale military exercises near Russia\\r...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.433664e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>http://freebeacon.com/national-security/stratc...</td>\n",
       "      <td>t1_cry9m2s</td>\n",
       "      <td>30</td>\n",
       "      <td>t3_38vt0w</td>\n",
       "      <td>/r/worldnews/comments/38vt0w/stratcom_deploys_...</td>\n",
       "      <td>-6</td>\n",
       "      <td>False</td>\n",
       "      <td>r/worldnews</td>\n",
       "      <td>public</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>t1_cv3psup</td>\n",
       "      <td>True</td>\n",
       "      <td>Clawisma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>That's the best I have seen today!\\r\\ncrying w...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.442426e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>http://i.imgur.com/mAbIDsm.gifv</td>\n",
       "      <td>t1_cv3psup</td>\n",
       "      <td>2129</td>\n",
       "      <td>t3_3l64el</td>\n",
       "      <td>/r/aww/comments/3l64el/wife_surprises_him_for_...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>r/aww</td>\n",
       "      <td>public</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>t1_dsije4z</td>\n",
       "      <td>False</td>\n",
       "      <td>picnicshirt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This is a pro-BitcoinCash article hosted on bi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.515670e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>https://news.bitcoin.com/miami-bitcoin-confere...</td>\n",
       "      <td>t1_dsije4z</td>\n",
       "      <td>47</td>\n",
       "      <td>t3_7pigps</td>\n",
       "      <td>/r/Buttcoin/comments/7pigps/miami_bitcoin_conf...</td>\n",
       "      <td>-2</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Buttcoin</td>\n",
       "      <td>public</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fullname  archived        author.name author_flair_text  \\\n",
       "5   t1_d509t9i      True        BlackToLive               NaN   \n",
       "10  t1_crya21g      True  FaurnFlamebreaker               NaN   \n",
       "11  t1_cry9m2s      True  FaurnFlamebreaker               NaN   \n",
       "37  t1_cv3psup      True           Clawisma               NaN   \n",
       "93  t1_dsije4z     False        picnicshirt               NaN   \n",
       "\n",
       "                                                 body  controversiality  \\\n",
       "5   By submitting to an independent, non-profit co...                 1   \n",
       "10  Well, in this country it's either you are top ...                 1   \n",
       "11  > large-scale military exercises near Russia\\r...                 1   \n",
       "37  That's the best I have seen today!\\r\\ncrying w...                 1   \n",
       "93  This is a pro-BitcoinCash article hosted on bi...                 1   \n",
       "\n",
       "     created_utc  distinguished  downs edited ...  \\\n",
       "5   1.467742e+09            NaN      0  False ...   \n",
       "10  1.433665e+09            NaN      0  False ...   \n",
       "11  1.433664e+09            NaN      0  False ...   \n",
       "37  1.442426e+09            NaN      0  False ...   \n",
       "93  1.515670e+09            NaN      0  False ...   \n",
       "\n",
       "                                             link_url        name  \\\n",
       "5                      http://blacktolive.org/?p=9589  t1_d509t9i   \n",
       "10  http://www.npr.org/2015/06/06/412314705/jury-a...  t1_crya21g   \n",
       "11  http://freebeacon.com/national-security/stratc...  t1_cry9m2s   \n",
       "37                    http://i.imgur.com/mAbIDsm.gifv  t1_cv3psup   \n",
       "93  https://news.bitcoin.com/miami-bitcoin-confere...  t1_dsije4z   \n",
       "\n",
       "    num_comments   parent_id  \\\n",
       "5              7  t1_d508xpp   \n",
       "10           287   t3_38v84b   \n",
       "11            30   t3_38vt0w   \n",
       "37          2129   t3_3l64el   \n",
       "93            47   t3_7pigps   \n",
       "\n",
       "                                            permalink score stickied  \\\n",
       "5   /r/politics/comments/4rdu5x/hillarys_war_on_bl...     0    False   \n",
       "10  /r/worldnews/comments/38v84b/jury_acquits_exbp...     0    False   \n",
       "11  /r/worldnews/comments/38vt0w/stratcom_deploys_...    -6    False   \n",
       "37  /r/aww/comments/3l64el/wife_surprises_him_for_...     1    False   \n",
       "93  /r/Buttcoin/comments/7pigps/miami_bitcoin_conf...    -2    False   \n",
       "\n",
       "   subreddit.display_name_prefixed subreddit_type ups  \n",
       "5                       r/politics         public   0  \n",
       "10                     r/worldnews         public   0  \n",
       "11                     r/worldnews         public  -6  \n",
       "37                           r/aww         public   1  \n",
       "93                      r/Buttcoin         public  -2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#New dataframe with controveriality > 0 printing comments\n",
    "controversial_comments = dataframe_comments.query('controversiality > 0').copy()\n",
    "#print(controversial_comments['body'][0])\n",
    "\n",
    "'''\n",
    "for num in controversial_comments.index.values:\n",
    "    print('num = ', num)\n",
    "    print(controversial_comments['body'][num], \"\\n\")\n",
    "'''\n",
    "\n",
    "controversial_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullname</th>\n",
       "      <th>archived</th>\n",
       "      <th>author.name</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>brand_safe</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>domain</th>\n",
       "      <th>downs</th>\n",
       "      <th>edited</th>\n",
       "      <th>...</th>\n",
       "      <th>selftext</th>\n",
       "      <th>spoiler</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit_name_prefixed</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>title</th>\n",
       "      <th>ups</th>\n",
       "      <th>url</th>\n",
       "      <th>whitelist_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_8b7ryg</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1.523369e+09</td>\n",
       "      <td>admin</td>\n",
       "      <td>self.reddit</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>u/reddit</td>\n",
       "      <td>user</td>\n",
       "      <td>0</td>\n",
       "      <td>This account is banned and is temporarily pres...</td>\n",
       "      <td>325</td>\n",
       "      <td>https://www.reddit.com/r/u_reddit/comments/8b7...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_511r3a</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1.472951e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Bad_Cop_No_Donut</td>\n",
       "      <td>public</td>\n",
       "      <td>125644</td>\n",
       "      <td>Cops really be trying hard to take non-black p...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://twitter.com/gloed_up/status/7707843365...</td>\n",
       "      <td>promo_adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_4xyikx</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>1.471337e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abcnews.go.com</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>r/news</td>\n",
       "      <td>public</td>\n",
       "      <td>15884252</td>\n",
       "      <td>Milwaukee Police Chief: Some Arrests Made but ...</td>\n",
       "      <td>1</td>\n",
       "      <td>http://abcnews.go.com/US/wireStory/latest-poli...</td>\n",
       "      <td>all_ads</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_4va15u</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1.469844e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>blacktolive.org</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>r/BlackLivesMatter</td>\n",
       "      <td>public</td>\n",
       "      <td>6676</td>\n",
       "      <td>We Want Real Equality</td>\n",
       "      <td>1</td>\n",
       "      <td>http://blacktolive.org/opinion/we-want-real-eq...</td>\n",
       "      <td>promo_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_4v9u9s</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1.469841e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>blacktolive.org</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Bad_Cop_No_Donut</td>\n",
       "      <td>public</td>\n",
       "      <td>125644</td>\n",
       "      <td>Three Remaining Cops In The Freddie Gray Case ...</td>\n",
       "      <td>1</td>\n",
       "      <td>http://blacktolive.org/news/crime/three-remain...</td>\n",
       "      <td>promo_adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    fullname  archived  author.name author_flair_text  brand_safe  \\\n",
       "0  t3_8b7ryg     False       reddit               NaN       False   \n",
       "1  t3_511r3a      True  BlackToLive               NaN       False   \n",
       "2  t3_4xyikx      True  BlackToLive               NaN        True   \n",
       "3  t3_4va15u      True  BlackToLive               NaN       False   \n",
       "4  t3_4v9u9s      True  BlackToLive               NaN       False   \n",
       "\n",
       "    created_utc distinguished           domain  downs edited  \\\n",
       "0  1.523369e+09         admin      self.reddit      0  False   \n",
       "1  1.472951e+09           NaN      twitter.com      0  False   \n",
       "2  1.471337e+09           NaN   abcnews.go.com      0  False   \n",
       "3  1.469844e+09           NaN  blacktolive.org      0  False   \n",
       "4  1.469841e+09           NaN  blacktolive.org      0  False   \n",
       "\n",
       "         ...         selftext spoiler  stickied  subreddit_name_prefixed  \\\n",
       "0        ...              NaN   False     False                 u/reddit   \n",
       "1        ...              NaN   False     False       r/Bad_Cop_No_Donut   \n",
       "2        ...              NaN   False     False                   r/news   \n",
       "3        ...              NaN   False     False       r/BlackLivesMatter   \n",
       "4        ...              NaN   False     False       r/Bad_Cop_No_Donut   \n",
       "\n",
       "   subreddit_type  subreddit_subscribers  \\\n",
       "0            user                      0   \n",
       "1          public                 125644   \n",
       "2          public               15884252   \n",
       "3          public                   6676   \n",
       "4          public                 125644   \n",
       "\n",
       "                                               title  ups  \\\n",
       "0  This account is banned and is temporarily pres...  325   \n",
       "1  Cops really be trying hard to take non-black p...    1   \n",
       "2  Milwaukee Police Chief: Some Arrests Made but ...    1   \n",
       "3                              We Want Real Equality    1   \n",
       "4  Three Remaining Cops In The Freddie Gray Case ...    1   \n",
       "\n",
       "                                                 url  whitelist_status  \n",
       "0  https://www.reddit.com/r/u_reddit/comments/8b7...               NaN  \n",
       "1  https://twitter.com/gloed_up/status/7707843365...       promo_adult  \n",
       "2  http://abcnews.go.com/US/wireStory/latest-poli...           all_ads  \n",
       "3  http://blacktolive.org/opinion/we-want-real-eq...         promo_all  \n",
       "4  http://blacktolive.org/news/crime/three-remain...       promo_adult  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_submissions = pd.read_csv('reddit-suspicious-accounts/data/submissions.csv', encoding='utf-8')\n",
    "dataframe_submissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullname</th>\n",
       "      <th>comment_karma</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>has_subscribed</th>\n",
       "      <th>has_verified_email</th>\n",
       "      <th>icon_img</th>\n",
       "      <th>id</th>\n",
       "      <th>is_employee</th>\n",
       "      <th>is_gold</th>\n",
       "      <th>is_mod</th>\n",
       "      <th>link_karma</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t2_z919g</td>\n",
       "      <td>-7</td>\n",
       "      <td>1.467681e+09</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>https://www.redditstatic.com/avatars/avatar_de...</td>\n",
       "      <td>z919g</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>BlackToLive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2_nhk4d</td>\n",
       "      <td>-5</td>\n",
       "      <td>1.431683e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.redditstatic.com/avatars/avatar_de...</td>\n",
       "      <td>nhk4d</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>FaurnFlamebreaker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t2_r8rca</td>\n",
       "      <td>-4</td>\n",
       "      <td>1.445054e+09</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.redditstatic.com/avatars/avatar_de...</td>\n",
       "      <td>r8rca</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Bill_Jonson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t2_nhml3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.431694e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.redditstatic.com/avatars/avatar_de...</td>\n",
       "      <td>nhml3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>PurebringerOghmagra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t2_qidqv</td>\n",
       "      <td>-2</td>\n",
       "      <td>1.442424e+09</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.redditstatic.com/avatars/avatar_de...</td>\n",
       "      <td>qidqv</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Clawisma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fullname  comment_karma   created_utc  has_subscribed has_verified_email  \\\n",
       "0  t2_z919g             -7  1.467681e+09            True               True   \n",
       "1  t2_nhk4d             -5  1.431683e+09           False              False   \n",
       "2  t2_r8rca             -4  1.445054e+09            True              False   \n",
       "3  t2_nhml3              0  1.431694e+09           False              False   \n",
       "4  t2_qidqv             -2  1.442424e+09            True              False   \n",
       "\n",
       "                                            icon_img     id  is_employee  \\\n",
       "0  https://www.redditstatic.com/avatars/avatar_de...  z919g        False   \n",
       "1  https://www.redditstatic.com/avatars/avatar_de...  nhk4d        False   \n",
       "2  https://www.redditstatic.com/avatars/avatar_de...  r8rca        False   \n",
       "3  https://www.redditstatic.com/avatars/avatar_de...  nhml3        False   \n",
       "4  https://www.redditstatic.com/avatars/avatar_de...  qidqv        False   \n",
       "\n",
       "   is_gold  is_mod  link_karma                 name  \n",
       "0    False   False           1          BlackToLive  \n",
       "1    False   False           1    FaurnFlamebreaker  \n",
       "2    False   False           1          Bill_Jonson  \n",
       "3    False   False           1  PurebringerOghmagra  \n",
       "4    False   False           1             Clawisma  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_users = pd.read_csv('reddit-suspicious-accounts/data/users.csv', encoding='utf-8')\n",
    "dataframe_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Plot top 10 most common words in upvoted comments'''\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "#Insignificant word filter\n",
    "stopwords = stopwords.words('english') #Insignificant word filter\n",
    "nonImportant = ['wa', 'one', 'like', 'dont', 'want', 'make', 'would', 'thats', 'think', 'really', 'way', \\\n",
    "    'get', 'im', 'u']\n",
    "stopwords.extend(nonImportant)\n",
    "lemmatizer = WordNetLemmatizer() #Lemmatizer instance\n",
    "upvoted_comments = dataframe_comments.query('ups >= 25').copy() #Gather comments with 10 or more upvotes\n",
    "tokenized_comments = [] #Tokenized comments\n",
    "\n",
    "#Tokenize each comment\n",
    "for itr in upvoted_comments.index.values:\n",
    "    lowercased = upvoted_comments['body'][itr].lower() #Lowercase all strings\n",
    "    #Strip punctuation\n",
    "    for punct in string.punctuation:\n",
    "        lowercased = lowercased.replace(punct, \"\")\n",
    "    tokenized_comments.append(nltk.word_tokenize(lowercased, language = 'English'))\n",
    "\n",
    "#Lemmatize each word\n",
    "for index in range(0, len(tokenized_comments)):\n",
    "    for w in range(0, len(tokenized_comments[index])):\n",
    "        tokenized_comments[index][w] = lemmatizer.lemmatize(tokenized_comments[index][w])\n",
    "        \n",
    "#Filter out insignificant words\n",
    "for i in range(0, len(tokenized_comments)):\n",
    "    tokenized_comments[i] = [word for word in tokenized_comments[i] if word not in stopwords]\n",
    "\n",
    "#Find counts of all words\n",
    "upvote_c = Counter()\n",
    "for lyst in tokenized_comments:\n",
    "    for werd in lyst:\n",
    "        upvote_c[werd] += 1\n",
    "\n",
    "top_upvote = upvote_c.most_common(10) #Get top 10 most frequent words\n",
    "\n",
    "#Plot in pie chart\n",
    "plt.pie([num[1] for num in top_upvote], labels = [w[0] for w in top_upvote], autopct = '%1.1f%%')\n",
    "plt.title(\">= 25 Upvotes\", bbox={'facecolor':'0.8', 'pad':5})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'one': 7, 'terrorist': 7, 'get': 6, 'make': 5, 'like': 5, 'people': 5, 'american': 4, 'wa': 4, 'dont': 4, 'bitcoin': 4, 'time': 4, 'could': 4, 'u': 4, 'even': 4, 'every': 3, 'want': 3, '1': 3, 'never': 3, 'account': 3, 'life': 3, 'next': 3, 'child': 3, 'well': 3, 'think': 3, 'muslim': 3, 'christian': 3, 'way': 3, 'post': 3, 'know': 3, 'cost': 2, 'thats': 2, 'actually': 2, 'feel': 2, 'casting': 2, 'two': 2, 'last': 2, 'movie': 2, 'friend': 2, 'food': 2, '000': 2, 'whats': 2, 'statement': 2, 'r': 2, 'create': 2, 'idea': 2, 'say': 2, 'ill': 2, 'roy': 2, 'moore': 2, 'good': 2, 'isnt': 2, 'meant': 2, 'obama': 2, 'trump': 2, 'call': 2, 'said': 2, 'america': 2, 'attack': 2, 'hall': 2, 'school': 2, 'fix': 2, 'cash': 2, 'old': 2, 'end': 2, 'russian': 2, 'hope': 2, 'killed': 2, 'illness': 2, 'doctor': 2, 'limited': 2, 'supply': 2, 'steal': 2, 'bf': 2, 'unfaithful': 2, 'bye': 2, 'funeral': 1, 'buried': 1, 'healthcare': 1, 'beef': 1, 'jerky': 1, 'delicious': 1, 'generally': 1, 'prohibitive': 1, 'numbness': 1, 'apathy': 1, 'hopeless': 1, 'hard': 1, 'motivate': 1, 'thing': 1, 'sadness': 1, 'separate': 1, 'emotion': 1, 'rarely': 1, 'generic': 1, 'white': 1, 'kid': 1, 'play': 1, 'sokka': 1, 'katara': 1, 'airbender': 1, 'scratch': 1, 'anyone': 1, 'pretty': 1, 'awful': 1, 'racist': 1, 'patient': 1, 'demanded': 1, 'specifically': 1, 'checked': 1, 'caucasian': 1, 'doctorand': 1, 'ethnicity': 1, 'country': 1, 'origin': 1, 'early': 1, '90': 1, 'internet': 1, 'illustration': 1, 'instruction': 1, 'tampon': 1, 'box': 1, 'oldest': 1, 'fucked': 1, 'pet': 1, 'german': 1, 'shepard': 1, 'staying': 1, 'hungry': 1, 'dad': 1, 'wearing': 1, 'mom': 1, 'dress': 1, 'absence': 1, 'proof': 1, 'seriously': 1, 'point': 1, 'unfounded': 1, 'george': 1, 'martin': 1, 'especially': 1, 'finish': 1, 'song': 1, 'ice': 1, 'fire': 1, 'instagram': 1, 'based': 1, 'unique': 1, 'popular': 1, 'money': 1, 'ad': 1, 'single': 1, 'cd': 1, 'entire': 1, 'boxed': 1, 'set': 1, 'note': 1, 'kirov': 1, 'reporting': 1, 'dropped': 1, 'childish': 1, 'gambino': 1, 'redbone': 1, 'beer': 1, 'snob': 1, 'look': 1, 'microbrew': 1, 'sometimes': 1, 'miller': 1, 'high': 1, 'chocolate': 1, 'eggnog': 1, 'stout': 1, 'ok': 1, 'eth': 1, 'change': 1, 'future': 1, 'build': 1, 'foundation': 1, 'dapps': 1, 'grow': 1, 'flourish': 1, 'ether': 1, 'currency': 1, 'doe': 1, 'value': 1, 'always': 1, 'forget': 1, 'quick': 1, 'gainz': 1, 'new': 1, 'paradigm': 1, 'barack': 1, 'donald': 1, 'recorded': 1, 'robocalls': 1, 'alabama': 1, 'senate': 1, 'race': 1, 'former': 1, 'federal': 1, 'prosecutor': 1, 'doug': 1, 'jones': 1, 'accused': 1, 'molester': 1, 'enough': 1, 'alrady': 1, 'great': 1, 'slavery': 1, 'else': 1, 'expect': 1, 'domestic': 1, 'immigrant': 1, 'holding': 1, 'btc': 1, 'extremely': 1, 'nervous': 1, '100': 1, 'main': 1, 'question': 1, 'come': 1, 'mind': 1, 'europe': 1, 'finally': 1, 'tired': 1, 'constant': 1, 'threat': 1, 'agree': 1, 'completely': 1, 'support': 1, 'opinion': 1, 'absolutely': 1, 'besides': 1, 'bad': 1, 'listen': 1, 'carol': 1, 'sitting': 1, 'vise': 1, 'versa': 1, 'visit': 1, 'festival': 1, 'concert': 1, 'true': 1, 'religiously': 1, 'tolerant': 1, 'society': 1, 'banning': 1, 'holiday': 1, 'wiretapping': 1, 'citizen': 1, 'necessary': 1, 'security': 1, 'l': 1, 'form': 1, 'tetri': 1, 'bfg': 1, '9000': 1, 'ridiculous': 1, 'piece': 1, 'nonsense': 1, 'ha': 1, 'involved': 1, 'insupported': 1, '2': 1, 'attempt': 1, 'xt': 1, 'classic': 1, 'oh': 1, 'apart': 1, 'trying': 1, 'gain': 1, 'leaverage': 1, 'conviction': 1, 'selling': 1, 'firecracker': 1, 'making': 1, 'mtgox': 1, 'accurate': 1, 'part': 1, 'poorly': 1, 'disguised': 1, 'program': 1, 'trust': 1, 'roger': 1, 'take': 1, 'idiot': 1, 'rubbish': 1, 'steemit': 1, 'grew': 1, 'execution': 1, 'romanov': 1, 'family': 1, 'july': 1, '1617': 1, '1918': 1, 'empire': 1, 'beginning': 1, 'soviet': 1, 'union': 1, 'guy': 1, 'back': 1, 'mess': 1, 'cat': 1, 'tail': 1, 'lowering': 1, 'strike': 1, 'youre': 1, 'right': 1, 'system': 1, 'corrupt': 1, 'dependable': 1, 'learn': 1, 'defend': 1, 'law': 1, 'realized': 1, 'cop': 1, 'afraid': 1, 'lawyer': 1, 'bring': 1, 'club': 1, 'elementary': 1, 'cant': 1, 'satanist': 1, 'need': 1, 'gun': 1, 'protect': 1, 'property': 1, 'hangover': 1, 'morning': 1, 'without': 1, 'alcohol': 1, 'creating': 1, 'video': 1, 'game': 1, 'angrier': 1, 'playing': 1, 'drill': 1, 'hole': 1, 'iphone': 1, 'hidden': 1, 'headphone': 1, 'socket': 1, 'cover': 1, 'hereby': 1, 'deusxyx': 1, 'promise': 1, 'use': 1, 'smth': 1, 'abbreviation': 1, 'something': 1, 'drive': 1, 'abyss': 1, 'obscurity': 1, 'offer': 1, 'sincere': 1, 'apology': 1, 'rdesignporn': 1, 'member': 1, 'mistake': 1, 'forgiven': 1, 'walk': 1, 'sunset': 1, 'sackcloth': 1, 'ash': 1, 'lie': 1, 'calmly': 1, 'baby': 1, 'breastfeeds': 1, 'caress': 1, 'im': 1, 'enjoying': 1, 'soon': 1, 'picture': 1, 'pose': 1, 'used': 1, 'obituary': 1, 'first': 1, 'firefighter': 1, 'responding': 1, '911': 1, 'struck': 1, 'dead': 1, 'courtyard': 1, 'falling': 1, 'body': 1, 'simultaneously': 1, 'huge': 1, 'amount': 1, 'arent': 1, 'curable': 1, 'treatable': 1, 'go': 1, 'find': 1, 'wrong': 1, 'many': 1, 'throw': 1, 'hand': 1, 'causing': 1, 'unwell': 1, 'often': 1, 'year': 1, 'become': 1, 'astronaut': 1, 'apocalyptic': 1, 'asteroid': 1, 'hit': 1, 'among': 1, 'human': 1, 'left': 1, 'alive': 1, 'oxygen': 1, 'external': 1, 'assistance': 1, 'returning': 1, 'home': 1, 'surviving': 1, 'may': 1, 'youve': 1, 'gone': 1, 'insane': 1, 'previous': 1, 'aka': 1, 'typical': 1, 'bestof': 1, '4': 1, 'day': 1, 'link': 1, 'month': 1, 'complains': 1, 'bot': 1, 'downvotes': 1, 'etc': 1, 'insta': 1, 'upvotes': 1, 'frontpage': 1, 'kinda': 1, 'obvious': 1, 'exactly': 1, 'spread': 1, 'misinformation': 1, 'narrative': 1, 'much': 1, 'utorrent': 1, '2017': 1, 'lul': 1, 'vote': 1, 'wallet': 1, 'vac': 1, 'httpsenwikipediaorgwikiembraceextendandextinguish': 1, 'try': 1, 'wider': 1, '1920': 1, 'horizontal': 1, 'scrolling': 1, 'cyanogen': 1, 'another': 1, 'httpiimgurcomgzpymfjjpg': 1, 'rcringe': 1})\n",
      "[('one', 7), ('terrorist', 7), ('get', 6), ('make', 5), ('like', 5), ('people', 5), ('american', 4), ('wa', 4), ('dont', 4), ('bitcoin', 4)]\n"
     ]
    }
   ],
   "source": [
    "'''Analyze upvoted comments'''\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "stopwords = stopwords.words('english') #Insignificant word filter\n",
    "\n",
    "#aiming to remove inflectional endings only and to return the base or dictionary form of a word\n",
    "lemmatizer = WordNetLemmatizer() #Lemmatizer instance\n",
    "\n",
    "upvoted_comments = dataframe_comments.query('ups >= 50').copy() #Gather comments with (50) or more upvotes\n",
    "tokenized_comments = [] #Tokenized comments\n",
    "\n",
    "#Tokenize each comment\n",
    "for itr in upvoted_comments.index.values:\n",
    "    lowercased = upvoted_comments['body'][itr].lower() #Lowercase all strings\n",
    "    #Strip punctuation\n",
    "    for punct in string.punctuation:\n",
    "        lowercased = lowercased.replace(punct, \"\")\n",
    "    tokenized_comments.append(nltk.word_tokenize(lowercased, language = 'English'))\n",
    "\n",
    "#Lemmatize each word\n",
    "for index in range(0, len(tokenized_comments)):\n",
    "    for w in range(0, len(tokenized_comments[index])):\n",
    "        tokenized_comments[index][w] = lemmatizer.lemmatize(tokenized_comments[index][w])\n",
    "        \n",
    "#Filter out insignificant words\n",
    "for i in range(0, len(tokenized_comments)):\n",
    "    tokenized_comments[i] = [word for word in tokenized_comments[i] if word not in stopwords]\n",
    "\n",
    "#Find counts of all words\n",
    "c = Counter()\n",
    "for lyst in tokenized_comments:\n",
    "    for werd in lyst:\n",
    "        c[werd] += 1\n",
    "    \n",
    "print(c)\n",
    "print(c.most_common(10))\n",
    "\n",
    "'''\n",
    "Need to: compare most common words for all comments and downvoted comments.\n",
    "Find Subreddits that posts were most popular in\n",
    "Possibly some sort of clustering\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('a', 'lot', 'of'), 62), (('one', 'of', 'the'), 55), (('ethan', 'bradberry', 'ethan'), 54), (('bradberry', 'ethan', 'bradberry'), 54), (('f', 'u', 'c'), 40), (('u', 'c', 'k'), 40), (('c', 'k', 'm'), 40), (('k', 'm', 'u'), 40), (('m', 'u', 's'), 40), (('u', 's', 'l'), 40), (('s', 'l', 'i'), 40), (('l', 'i', 'm'), 40), (('i', 'm', 's'), 40), (('retarded', 'faggot', 'retarded'), 40), (('faggot', 'retarded', 'faggot'), 40), (('m', 's', 'f'), 39), (('s', 'f', 'u'), 39), (('agree', 'with', 'you'), 38), (('it', '’', 's'), 36), (('i', 'am', 'not'), 35), (('i', 'agree', 'with'), 34), (('this', 'is', 'a'), 31), (('this', 'is', 'the'), 29), (('there', 'is', 'no'), 28), (('i', 'dont', 'think'), 27), (('there', 'is', 'a'), 25), (('i', 'do', 'not'), 24), (('do', 'you', 'think'), 23), (('is', 'going', 'to'), 23), (('it', 'will', 'be'), 22), (('i', 'want', 'to'), 22), (('i', 'think', 'that'), 22), (('is', 'not', 'a'), 21), (('it', 'wa', 'a'), 21), (('i', 'think', 'it'), 21), (('is', 'one', 'of'), 20), (('it', 'would', 'be'), 20), (('going', 'to', 'be'), 20), (('i', '’', 'm'), 20), (('i', 'dont', 'know'), 19), (('i', 'like', 'the'), 19), (('are', 'going', 'to'), 18), (('to', 'be', 'a'), 18), (('need', 'to', 'be'), 18), (('in', 'the', 'world'), 18), (('lot', 'of', 'people'), 18), (('it', 'look', 'like'), 18), (('what', 'do', 'you'), 18), (('by', 'the', 'way'), 18), (('if', 'you', 'want'), 18), (('you', 'want', 'to'), 18), (('out', 'of', 'the'), 17), (('if', 'you', 'have'), 17), (('to', 'be', 'the'), 17), (('be', 'able', 'to'), 17), (('don', '’', 't'), 17), (('the', 'end', 'of'), 16), (('seems', 'to', 'be'), 16), (('the', 'fact', 'that'), 16), (('the', 'same', 'time'), 16), (('the', 'rest', 'of'), 16), (('this', 'is', 'not'), 16), (('you', 'have', 'to'), 16), (('it', 'is', 'a'), 16), (('i', 'would', 'like'), 15), (('most', 'of', 'the'), 15), (('a', 'couple', 'of'), 15), (('when', 'i', 'wa'), 15), (('it', 'just', 'a'), 15), (('know', 'how', 'to'), 14), (('would', 'like', 'to'), 14), (('thanks', 'for', 'the'), 14), (('is', 'not', 'the'), 14), (('i', 'would', 'recommend'), 14), (('if', 'you', 'are'), 13), (('will', 'be', 'the'), 13), (('thanks', 'for', 'posting'), 13), (('at', 'the', 'same'), 13), (('thank', 'you', 'for'), 13), (('of', 'the', 'most'), 13), (('part', 'of', 'the'), 13), (('in', 'the', 'first'), 13), (('is', 'the', 'best'), 13), (('salt', 'and', 'tie'), 13), (('go', 'to', 'the'), 13), (('we', 'have', 'to'), 12), (('a', 'little', 'bit'), 12), (('first', 'of', 'all'), 12), (('we', 'need', 'to'), 12), (('the', 'first', 'place'), 12), (('of', 'the', 'world'), 12), (('it', 'can', 'be'), 12), (('in', 'the', 'middle'), 11), (('a', 'long', 'time'), 11), (('that', 'would', 'be'), 11), (('dont', 'want', 'to'), 11), (('that', '’', 's'), 11), (('it', 'is', 'not'), 11), (('and', 'i', 'dont'), 11), (('to', 'invest', 'in'), 11)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesse\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:50: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    }
   ],
   "source": [
    "'''Seach for Common Phrases in Comments Using Ngrams'''\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk import tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "from collections import Counter\n",
    "import string\n",
    "from pprint import pprint\n",
    "\n",
    "stopwords = stopwords.words('english') #Insignificant word filter\n",
    "\n",
    "#aiming to remove inflectional endings only and to return the base or dictionary form of a word\n",
    "lemmatizer = WordNetLemmatizer() #Lemmatizer instance\n",
    "\n",
    "#All Comments Tokenized\n",
    "tokenized_comments_all = []\n",
    "\n",
    "#Tokenize each comment\n",
    "for itr in dataframe_comments.index.values:\n",
    "    lowercased = dataframe_comments['body'][itr].lower() #Lowercase all strings\n",
    "    #Strip punctuation\n",
    "    for punct in string.punctuation:\n",
    "        lowercased = lowercased.replace(punct, \"\")\n",
    "    tokenized_comments_all.append(nltk.word_tokenize(lowercased, language = 'English'))\n",
    "\n",
    "#Lemmatize each word\n",
    "for index in range(0, len(tokenized_comments_all)):\n",
    "    for w in range(0, len(tokenized_comments_all[index])):\n",
    "        tokenized_comments_all[index][w] = lemmatizer.lemmatize(tokenized_comments_all[index][w])\n",
    "\n",
    "\n",
    "#Create n-grams of num_tokens_to_gram tokens\n",
    "num_tokens_to_gram = 3\n",
    "\n",
    "n_gram_tokens_all = []\n",
    "\n",
    "for num in range(0, len(tokenized_comments_all)):\n",
    "    n_gram_tokens = ngrams(tokenized_comments_all[num], num_tokens_to_gram)\n",
    "    n_gram_tokens_all.append(n_gram_tokens)\n",
    "#     if num < 10:\n",
    "#        print(Counter(n_gram_tokens))\n",
    "\n",
    "#Find counts of identical tokens\n",
    "c = Counter()\n",
    "for lyst in n_gram_tokens_all:\n",
    "    for gram in lyst:\n",
    "        c[gram] += 1\n",
    "    \n",
    "#print(c)\n",
    "print(c.most_common(100))\n",
    "\n",
    "# print(n_gram_3_tokens_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment Analysis of Comments Using Vader\n",
    "sia = SIA()\n",
    "comments_list = []\n",
    "for itr in dataframe_comments.index.values:\n",
    "    comments_list.append(dataframe_comments['body'][itr])\n",
    "\n",
    "results = []\n",
    "for itr in comments_list:\n",
    "    #print(itr)\n",
    "    pol_score = sia.polarity_scores(itr)\n",
    "    results.append(pol_score)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullname</th>\n",
       "      <th>archived</th>\n",
       "      <th>author.name</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>downs</th>\n",
       "      <th>edited</th>\n",
       "      <th>...</th>\n",
       "      <th>name</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>permalink</th>\n",
       "      <th>score</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit.display_name_prefixed</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>ups</th>\n",
       "      <th>polarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t1_d687zh5</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A hard look at training and tactics\" = They wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.470604e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>t1_d687zh5</td>\n",
       "      <td>119</td>\n",
       "      <td>t3_4wkn7m</td>\n",
       "      <td>/r/Bad_Cop_No_Donut/comments/4wkn7m/chicago_po...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Bad_Cop_No_Donut</td>\n",
       "      <td>public</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.097, 'neu': 0.903, 'pos': 0.0, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t1_d5wqzhx</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>They deserve all of the hate</td>\n",
       "      <td>0</td>\n",
       "      <td>1.469847e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>t1_d5wqzhx</td>\n",
       "      <td>96</td>\n",
       "      <td>t3_4v5xpc</td>\n",
       "      <td>/r/Bad_Cop_No_Donut/comments/4v5xpc/arkansas_p...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Bad_Cop_No_Donut</td>\n",
       "      <td>public</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.425, 'neu': 0.575, 'pos': 0.0, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t1_d5qvqfw</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I guess that's what they mean when say \"I don'...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.469498e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>t1_d5qvqfw</td>\n",
       "      <td>210</td>\n",
       "      <td>t1_d5qeyrw</td>\n",
       "      <td>/r/Bad_Cop_No_Donut/comments/4uiezg/no_charges...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Bad_Cop_No_Donut</td>\n",
       "      <td>public</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t1_d5quz9y</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It's never too late for them, It's never too c...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.469497e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>t1_d5quz9y</td>\n",
       "      <td>18</td>\n",
       "      <td>t3_4uicjv</td>\n",
       "      <td>/r/Bad_Cop_No_Donut/comments/4uicjv/police_off...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Bad_Cop_No_Donut</td>\n",
       "      <td>public</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.151, 'neu': 0.736, 'pos': 0.113, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t1_d565ls1</td>\n",
       "      <td>True</td>\n",
       "      <td>BlackToLive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://petitions.whitehouse.gov//petition/pet...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.468114e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>t1_d565ls1</td>\n",
       "      <td>12</td>\n",
       "      <td>t1_d55o1gr</td>\n",
       "      <td>/r/Good_Cop_Free_Donut/comments/4s0s3j/you_can...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>r/Good_Cop_Free_Donut</td>\n",
       "      <td>public</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     fullname  archived  author.name author_flair_text  \\\n",
       "0  t1_d687zh5      True  BlackToLive               NaN   \n",
       "1  t1_d5wqzhx      True  BlackToLive               NaN   \n",
       "2  t1_d5qvqfw      True  BlackToLive               NaN   \n",
       "3  t1_d5quz9y      True  BlackToLive               NaN   \n",
       "4  t1_d565ls1      True  BlackToLive               NaN   \n",
       "\n",
       "                                                body  controversiality  \\\n",
       "0  A hard look at training and tactics\" = They wi...                 0   \n",
       "1                       They deserve all of the hate                 0   \n",
       "2  I guess that's what they mean when say \"I don'...                 0   \n",
       "3  It's never too late for them, It's never too c...                 0   \n",
       "4  https://petitions.whitehouse.gov//petition/pet...                 0   \n",
       "\n",
       "    created_utc  distinguished  downs edited  \\\n",
       "0  1.470604e+09            NaN      0  False   \n",
       "1  1.469847e+09            NaN      0  False   \n",
       "2  1.469498e+09            NaN      0  False   \n",
       "3  1.469497e+09            NaN      0  False   \n",
       "4  1.468114e+09            NaN      0  False   \n",
       "\n",
       "                         ...                                name num_comments  \\\n",
       "0                        ...                          t1_d687zh5          119   \n",
       "1                        ...                          t1_d5wqzhx           96   \n",
       "2                        ...                          t1_d5qvqfw          210   \n",
       "3                        ...                          t1_d5quz9y           18   \n",
       "4                        ...                          t1_d565ls1           12   \n",
       "\n",
       "    parent_id                                          permalink score  \\\n",
       "0   t3_4wkn7m  /r/Bad_Cop_No_Donut/comments/4wkn7m/chicago_po...     1   \n",
       "1   t3_4v5xpc  /r/Bad_Cop_No_Donut/comments/4v5xpc/arkansas_p...     1   \n",
       "2  t1_d5qeyrw  /r/Bad_Cop_No_Donut/comments/4uiezg/no_charges...     1   \n",
       "3   t3_4uicjv  /r/Bad_Cop_No_Donut/comments/4uicjv/police_off...     1   \n",
       "4  t1_d55o1gr  /r/Good_Cop_Free_Donut/comments/4s0s3j/you_can...     1   \n",
       "\n",
       "  stickied subreddit.display_name_prefixed subreddit_type ups  \\\n",
       "0    False              r/Bad_Cop_No_Donut         public   1   \n",
       "1    False              r/Bad_Cop_No_Donut         public   1   \n",
       "2    False              r/Bad_Cop_No_Donut         public   1   \n",
       "3    False              r/Bad_Cop_No_Donut         public   1   \n",
       "4    False           r/Good_Cop_Free_Donut         public   1   \n",
       "\n",
       "                                      polarity_score  \n",
       "0  {'neg': 0.097, 'neu': 0.903, 'pos': 0.0, 'comp...  \n",
       "1  {'neg': 0.425, 'neu': 0.575, 'pos': 0.0, 'comp...  \n",
       "2  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  \n",
       "3  {'neg': 0.151, 'neu': 0.736, 'pos': 0.113, 'co...  \n",
       "4  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sentiment Analysis of Comments Using Vader\n",
    "sia = SIA()\n",
    "comments_list = []\n",
    "for itr in dataframe_comments.index.values:\n",
    "    comments_list.append(dataframe_comments['body'][itr])\n",
    "\n",
    "results = []\n",
    "for itr in comments_list:\n",
    "    pol_score = sia.polarity_scores(itr)\n",
    "    results.append(pol_score)\n",
    "\n",
    "#df1 = DataFrame({'polarity_score': results})\n",
    "df1 = pd.DataFrame({'polarity_score': results})\n",
    "\n",
    "#Added Polarity Scores to dataframe_comments\n",
    "dataframe_comments['polarity_score'] = df1\n",
    "\n",
    "dataframe_comments.head()\n",
    "#print(dataframe_comments['polarity_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "% of posts and comments at subreddit, maybe per account per subreddit\n",
    "        Where they are posting and commenting\n",
    "'''\n",
    "\n",
    "'''\n",
    "subreddit_name_prefixed\n",
    "subreddit_type\n",
    "subreddit_subscribers\n",
    "'''\n",
    "\n",
    "dataframe_submissions = pd.read_csv('reddit-suspicious-accounts/data/submissions.csv', encoding='utf-8')\n",
    "#dataframe_submissions.head()\n",
    "\n",
    "#Separate Dataframes Based on subreddit_type\n",
    "print(len(dataframe_submissions))\n",
    "\n",
    "# for itr in dataframe_submissions.index.values:\n",
    "#     lowercased = dataframe_comments['body'][itr]\n",
    "#     #Strip punctuation\n",
    "#     for punct in string.punctuation:\n",
    "#         lowercased = lowercased.replace(punct, \"\")\n",
    "#     tokenized_comments_all.append(nltk.word_tokenize(lowercased, language = 'English'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import collections\n",
    " \n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    " \n",
    " \n",
    "def process_text(text, stem=True):\n",
    "    \"\"\" Tokenize text and stem words removing punctuation \"\"\"\n",
    "    text = text.translate(None, string.punctuation)\n",
    "    tokens = word_tokenize(text)\n",
    " \n",
    "    if stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    " \n",
    "    return tokens\n",
    " \n",
    " \n",
    "def cluster_texts(texts, clusters=3):\n",
    "    \"\"\" Transform texts to Tf-Idf coordinates and cluster texts using K-Means \"\"\"\n",
    "    vectorizer = TfidfVectorizer(tokenizer=process_text,\n",
    "                                 stop_words=stopwords.words('english'),\n",
    "                                 max_df=0.5,\n",
    "                                 min_df=0.1,\n",
    "                                 lowercase=True)\n",
    " \n",
    "    tfidf_model = vectorizer.fit_transform(texts)\n",
    "    km_model = KMeans(n_clusters=clusters)\n",
    "    km_model.fit(tfidf_model)\n",
    " \n",
    "    clustering = collections.defaultdict(list)\n",
    " \n",
    "    for idx, label in enumerate(km_model.labels_):\n",
    "        clustering[label].append(idx)\n",
    " \n",
    "    return clustering\n",
    " \n",
    " \n",
    "if __name__ == \"__main__\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
